
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>LightGBM Handbook &#8212; Colabで試せるPythonライブラリTips集</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/LightGBM_handbook';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="shap" href="shap_handbook.html" />
    <link rel="prev" title="Plotly Tips Collection" href="plotly_tips_collection.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../my-intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/notebookgallerylogo.png" class="logo__image only-light" alt="Colabで試せるPythonライブラリTips集 - Home"/>
    <script>document.write(`<img src="../_static/notebookgallerylogo.png" class="logo__image only-dark" alt="Colabで試せるPythonライブラリTips集 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../my-intro.html">
                    <no title>
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="pandas_tips_collection.html">Pandas</a></li>
<li class="toctree-l1"><a class="reference internal" href="matplotlib_seaborn_tips_collection.html">Matplotlib Seaborn Tips Collection</a></li>



<li class="toctree-l1"><a class="reference internal" href="plotly_tips_collection.html">Plotly Tips Collection</a></li>









<li class="toctree-l1 current active"><a class="current reference internal" href="#">LightGBM Handbook</a></li>




<li class="toctree-l1"><a class="reference internal" href="shap_handbook.html">SHAP</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notebooks/LightGBM_handbook.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>LightGBM Handbook</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">LightGBM Handbook</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">基本</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#python-api">Python API</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">パラメータ</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lightgbm-train">lightgbm.train：学習してモデル作成</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#booster">Booster：モデル</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lightgbm-cv">lightgbm.cv：交差検証</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cvbooster">CVBooster：交差検証時のモデル群</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scikit-learn-api">scikit-learn API</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">注意点</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">2種類の重要度</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">欠損の扱い</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">再現性の確保</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">バージョンによる違い</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">可視化などの便利関数</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-metric">plot_metric：評価値の推移の可視化</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-api">train APIの場合</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sklearn-api">sklearn APIの場合</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-importance">plot_importance：重要度の可視化</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-split-value-histogram">plot_split_value_histogram：分岐に使われる値の可視化</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#get-split-value-histogram">get_split_value_histogram：分岐に使われる値の取得</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-tree">plot_tree：決定木の可視化</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trees-to-dataframe">trees_to_dataframe：決定木をテーブル形式で取得</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#optuna">Optuna：ハイパーパラメータ自動チューニング</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lightgbmtuner">LightGBMTuner</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lightgbmtunercv">LightGBMTunerCV</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optuna-integration-lightgbm-tain">optuna.integration.lightgbm.tain</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optuna-lightgbm">optuna.lightgbm 再現性</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optuna-inetgration-scikit-learn-api">optuna.inetgration scikit-learn API</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><a href="https://colab.research.google.com/github/abay-qkt/lightgbm-handbook/blob/main/LightGBM_handbook.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<section class="tex2jax_ignore mathjax_ignore" id="lightgbm-handbook">
<h1>LightGBM Handbook<a class="headerlink" href="#lightgbm-handbook" title="Link to this heading">#</a></h1>
<p>私がLightGBM使う上で調べたことをまとめてみました！<br />
基本的な使い方や注意事項、便利関数、自動パラメータチューニングについて書いています。</p>
<p>LightGBMについて基本的なことは知ってる人はチートシート的に見ていただけると思います！
基本的な機能だけでなく、LightGBMを使う上での注意点だったり、可視化の便利関数だったり普通に使っていても気づきづらいことも掲載しています。どれか一つでもお役に立てば幸いです！</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">lightgbm</span><span class="o">==</span><span class="mf">4.5.0</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">optuna</span><span class="o">-</span><span class="n">integration</span>
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1>基本<a class="headerlink" href="#id1" title="Link to this heading">#</a></h1>
<section id="python-api">
<h2>Python API<a class="headerlink" href="#python-api" title="Link to this heading">#</a></h2>
<p>lightGBMに関して、公式ドキュメントのこちらのページにPython APIがまとめて記載されています。<br />
<a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/Python-API.html">https://lightgbm.readthedocs.io/en/latest/Python-API.html</a></p>
<p>プロット関数もまとめてこのページに書いてあります。</p>
</section>
<section id="id2">
<h2>パラメータ<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<p>パラメータ一覧はこのページに書いてあります。</p>
<p><a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#core-parameters">https://lightgbm.readthedocs.io/en/latest/Parameters.html#core-parameters</a></p>
<p>こちらの記事で図解されていてとても分かりやすいです。<br />
<a class="reference external" href="https://knknkn.hatenablog.com/entry/2021/06/29/125226">https://knknkn.hatenablog.com/entry/2021/06/29/125226</a></p>
</section>
<section id="lightgbm-train">
<h2>lightgbm.train：学習してモデル作成<a class="headerlink" href="#lightgbm-train" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.train.html">https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.train.html</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">lightgbm</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lgb</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.datasets</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_x</span><span class="p">,</span> <span class="n">val_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">val_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dtrain</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">train_y</span><span class="p">)</span>
<span class="n">dval</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">val_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">val_y</span><span class="p">)</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;objective&quot;</span><span class="p">:</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span>
    <span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="s2">&quot;binary_logloss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;verbosity&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="n">dtrain</span><span class="p">,</span>
    <span class="n">valid_sets</span><span class="o">=</span><span class="p">[</span><span class="n">dtrain</span><span class="p">,</span> <span class="n">dval</span><span class="p">],</span>
    <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">lgb</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">lgb</span><span class="o">.</span><span class="n">log_evaluation</span><span class="p">(</span><span class="mi">100</span><span class="p">)],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.11/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: 
Dask dataframe query planning is disabled because dask-expr is not installed.

You can install it with `pip install dask[dataframe]` or `conda install dask`.
This will raise in a future version.

  warnings.warn(msg, FutureWarning)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	training&#39;s binary_logloss: 0.000462198	valid_1&#39;s binary_logloss: 0.12469
Early stopping, best iteration is:
[41]	training&#39;s binary_logloss: 0.02643	valid_1&#39;s binary_logloss: 0.103654
</pre></div>
</div>
</div>
</div>
<section id="booster">
<h3>Booster：モデル<a class="headerlink" href="#booster" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html">https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html</a></p>
<p>Boosterには特徴量名や重要度などが格納されています。<br />
predictメソッドで推論を行えます。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 特徴量の名前</span>
<span class="n">model</span><span class="o">.</span><span class="n">feature_name</span><span class="p">()[:</span><span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;Column_0&#39;, &#39;Column_1&#39;, &#39;Column_2&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 重要度</span>
<span class="n">model</span><span class="o">.</span><span class="n">feature_importance</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([14, 78, 11,  9, 10, 10, 25, 43, 12,  9, 22, 36, 17, 49, 10, 17,  8,
       17, 24, 10, 30, 72, 39, 27, 13, 12, 25, 76, 49,  4], dtype=int32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 推論</span>
<span class="n">pred_y</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span>
<span class="n">pred_y</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.99404026, 0.00858318, 0.99150761, 0.00992074, 0.00969055])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="lightgbm-cv">
<h2><a class="reference external" href="http://lightgbm.cv">lightgbm.cv</a>：交差検証<a class="headerlink" href="#lightgbm-cv" title="Link to this heading">#</a></h2>
<p>lightgbm .cvを使うと、lightGBMで交差検証を簡単に行うことができます。<br />
ブーストラウンドごとに全foldの評価指標の平均と標準偏差が計算されます。<br />
自分で交差検証のコードを書く手間が省けるので便利です。</p>
<p><a class="reference external" href="http://lightgbm.cv">lightgbm.cv</a><br />
<a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.cv.html">https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.cv.html</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">lightgbm</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lgb</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.datasets</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dtrain</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;objective&quot;</span><span class="p">:</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span>
    <span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="s2">&quot;binary_logloss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;verbosity&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;boosting_type&quot;</span><span class="p">:</span> <span class="s2">&quot;gbdt&quot;</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">eval_results</span>  <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span>
    <span class="n">params</span><span class="p">,</span> <span class="n">dtrain</span><span class="p">,</span>
    <span class="n">nfold</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">stratified</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">lgb</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">lgb</span><span class="o">.</span><span class="n">log_evaluation</span><span class="p">(</span><span class="mi">10</span><span class="p">)],</span>
    <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">100</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 10 rounds
[10]	cv_agg&#39;s valid binary_logloss: 0.274529 + 0.0199056
[20]	cv_agg&#39;s valid binary_logloss: 0.165792 + 0.029172
[30]	cv_agg&#39;s valid binary_logloss: 0.117952 + 0.0315023
[40]	cv_agg&#39;s valid binary_logloss: 0.0975496 + 0.0326283
[50]	cv_agg&#39;s valid binary_logloss: 0.092958 + 0.0361704
[60]	cv_agg&#39;s valid binary_logloss: 0.0925361 + 0.0411463
Early stopping, best iteration is:
[52]	cv_agg&#39;s valid binary_logloss: 0.091621 + 0.0362409
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># ブーストラウンドごとに全foldの評価値の平均と標準偏差が辞書で返される</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">eval_results</span><span class="p">)</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># 最後の行が最終的な結果</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
  <div id="df-3d15719f-7865-4d28-97f1-46fb43a5d189" class="colab-df-container">
    <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>valid binary_logloss-mean</th>
      <th>valid binary_logloss-stdv</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>47</th>
      <td>0.092985</td>
      <td>0.035959</td>
    </tr>
    <tr>
      <th>48</th>
      <td>0.093120</td>
      <td>0.035929</td>
    </tr>
    <tr>
      <th>49</th>
      <td>0.092958</td>
      <td>0.036170</td>
    </tr>
    <tr>
      <th>50</th>
      <td>0.092147</td>
      <td>0.036070</td>
    </tr>
    <tr>
      <th>51</th>
      <td>0.091621</td>
      <td>0.036241</td>
    </tr>
  </tbody>
</table>
</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-3d15719f-7865-4d28-97f1-46fb43a5d189')"
            title="Convert this dataframe to an interactive table."
            style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewBox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"/>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-3d15719f-7865-4d28-97f1-46fb43a5d189 button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-3d15719f-7865-4d28-97f1-46fb43a5d189');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-bcc16e2c-f11a-45f3-9cbd-745606795053">
  <button class="colab-df-quickchart" onclick="quickchart('df-bcc16e2c-f11a-45f3-9cbd-745606795053')"
            title="Suggest charts"
            style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
     width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"/>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-bcc16e2c-f11a-45f3-9cbd-745606795053 button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>

    </div>
  </div>
</div></div>
</div>
<p>参考記事</p>
<ul class="simple">
<li><p><a class="reference external" href="https://stackoverflow.com/questions/66994779/light-gbm-regression-cv-interpreting-results">https://stackoverflow.com/questions/66994779/light-gbm-regression-cv-interpreting-results</a></p>
<ul>
<li><p>評価結果はfoldごとではなく、ブーストラウンドごとのfold平均と標準偏差しか出していない</p></li>
</ul>
</li>
</ul>
<section id="cvbooster">
<h3>CVBooster：交差検証時のモデル群<a class="headerlink" href="#cvbooster" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.CVBooster.html">https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.CVBooster.html</a></p>
<p>lightgbm.cvでreturn_cvboosterオプションをTrueにすると、交差検証時に作成されたモデルを取得することができます。</p>
<p>返されたモデル(CVBooster)でpredictメソッドを使うと各foldのBoosterによる予測結果をまとめて出力できます。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">lightgbm</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lgb</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.datasets</span>
<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dtrain</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;objective&quot;</span><span class="p">:</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span>
    <span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="s2">&quot;binary_logloss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;verbosity&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;boosting_type&quot;</span><span class="p">:</span> <span class="s2">&quot;gbdt&quot;</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">eval_results</span>  <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span>
    <span class="n">params</span><span class="p">,</span> <span class="n">dtrain</span><span class="p">,</span>
    <span class="n">nfold</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">stratified</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">lgb</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">lgb</span><span class="o">.</span><span class="n">log_evaluation</span><span class="p">(</span><span class="mi">10</span><span class="p">)],</span>
    <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">return_cvbooster</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 10 rounds
[10]	cv_agg&#39;s valid binary_logloss: 0.274529 + 0.0199056
[20]	cv_agg&#39;s valid binary_logloss: 0.165792 + 0.029172
[30]	cv_agg&#39;s valid binary_logloss: 0.117952 + 0.0315023
[40]	cv_agg&#39;s valid binary_logloss: 0.0975496 + 0.0326283
[50]	cv_agg&#39;s valid binary_logloss: 0.092958 + 0.0361704
[60]	cv_agg&#39;s valid binary_logloss: 0.0925361 + 0.0411463
Early stopping, best iteration is:
[52]	cv_agg&#39;s valid binary_logloss: 0.091621 + 0.0362409
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cvbooster</span> <span class="o">=</span> <span class="n">eval_results</span><span class="p">[</span><span class="s2">&quot;cvbooster&quot;</span><span class="p">]</span> <span class="c1"># lightgbm.CVBoosterが得られる</span>
<span class="n">pred_data</span> <span class="o">=</span> <span class="n">cvbooster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="c1"># 各foldのモデルによる予測結果が返される</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">pred_data</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> <span class="c1"># 各列が各foldの予測結果になるよう表示</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
  <div id="df-248cd708-83e7-4b88-9226-4c1975e669ba" class="colab-df-container">
    <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.013154</td>
      <td>0.012107</td>
      <td>0.010080</td>
      <td>0.016935</td>
      <td>0.014565</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.006946</td>
      <td>0.004671</td>
      <td>0.003985</td>
      <td>0.004670</td>
      <td>0.009152</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.003309</td>
      <td>0.003224</td>
      <td>0.003144</td>
      <td>0.003448</td>
      <td>0.003137</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.034154</td>
      <td>0.054067</td>
      <td>0.036073</td>
      <td>0.031005</td>
      <td>0.054038</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.013947</td>
      <td>0.033774</td>
      <td>0.018723</td>
      <td>0.020457</td>
      <td>0.026545</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>564</th>
      <td>0.004145</td>
      <td>0.003312</td>
      <td>0.005082</td>
      <td>0.004701</td>
      <td>0.003753</td>
    </tr>
    <tr>
      <th>565</th>
      <td>0.004064</td>
      <td>0.003383</td>
      <td>0.004702</td>
      <td>0.003900</td>
      <td>0.003883</td>
    </tr>
    <tr>
      <th>566</th>
      <td>0.004851</td>
      <td>0.005775</td>
      <td>0.012275</td>
      <td>0.009714</td>
      <td>0.011044</td>
    </tr>
    <tr>
      <th>567</th>
      <td>0.004058</td>
      <td>0.003376</td>
      <td>0.003335</td>
      <td>0.003357</td>
      <td>0.003519</td>
    </tr>
    <tr>
      <th>568</th>
      <td>0.995964</td>
      <td>0.996385</td>
      <td>0.997090</td>
      <td>0.997067</td>
      <td>0.996448</td>
    </tr>
  </tbody>
</table>
<p>569 rows × 5 columns</p>
</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-248cd708-83e7-4b88-9226-4c1975e669ba')"
            title="Convert this dataframe to an interactive table."
            style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewBox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"/>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-248cd708-83e7-4b88-9226-4c1975e669ba button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-248cd708-83e7-4b88-9226-4c1975e669ba');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-4968cbd2-cbc0-4e65-ad19-88562b8bfdda">
  <button class="colab-df-quickchart" onclick="quickchart('df-4968cbd2-cbc0-4e65-ad19-88562b8bfdda')"
            title="Suggest charts"
            style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
     width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"/>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-4968cbd2-cbc0-4e65-ad19-88562b8bfdda button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>

    </div>
  </div>
</div></div>
</div>
</section>
</section>
<section id="scikit-learn-api">
<h2>scikit-learn API<a class="headerlink" href="#scikit-learn-api" title="Link to this heading">#</a></h2>
<p>LightGBMにはscikit-learn APIが用意されていて、LGBMRegressorやLGBMClassifierを使うことで、<br />
scikit-learnのモデルと同じ感覚で回帰や分類ができます。<br />
LightGBMに慣れていない方や、scikit-learnのPipelineに組み込む場合に便利だと思います。</p>
<p><a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/Python-API.html#scikit-learn-api">https://lightgbm.readthedocs.io/en/latest/Python-API.html#scikit-learn-api</a></p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id3">
<h1>注意点<a class="headerlink" href="#id3" title="Link to this heading">#</a></h1>
<section id="id4">
<h2>2種類の重要度<a class="headerlink" href="#id4" title="Link to this heading">#</a></h2>
<p>重要度は2種類あり、importance_typeで指定できます</p>
<ul class="simple">
<li><p>split：分岐に使われた回数</p></li>
<li><p>gain：損失減少にどれだけ貢献したか</p></li>
</ul>
<p>ユニーク数が多い特徴量は損失減少にさほど貢献していなくとも分岐に頻繁に使われることがあるみたいなので、<br />
モデルの性能向上に寄与する特徴量を知りたいならgainを見るのが良さそう。<br />
デフォルトはsplitになっていることに注意です。</p>
<p>重要度に関してはこちらの記事がとても参考になります<br />
<a class="reference external" href="https://www.mcdigital.jp/blog/20240530techblog/">https://www.mcdigital.jp/blog/20240530techblog/</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">lightgbm</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lgb</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.datasets</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_x</span><span class="p">,</span> <span class="n">val_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">val_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dtrain</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">train_y</span><span class="p">)</span>
<span class="n">dval</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">val_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">val_y</span><span class="p">)</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;objective&quot;</span><span class="p">:</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span>
    <span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="s2">&quot;binary_logloss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;verbosity&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="n">dtrain</span><span class="p">,</span>
    <span class="n">valid_sets</span><span class="o">=</span><span class="p">[</span><span class="n">dtrain</span><span class="p">,</span> <span class="n">dval</span><span class="p">],</span>
    <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">lgb</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">lgb</span><span class="o">.</span><span class="n">log_evaluation</span><span class="p">(</span><span class="mi">100</span><span class="p">)],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	training&#39;s binary_logloss: 0.000462198	valid_1&#39;s binary_logloss: 0.12469
Early stopping, best iteration is:
[41]	training&#39;s binary_logloss: 0.02643	valid_1&#39;s binary_logloss: 0.103654
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 重要度　split</span>
<span class="n">model</span><span class="o">.</span><span class="n">feature_importance</span><span class="p">(</span><span class="n">importance_type</span><span class="o">=</span><span class="s1">&#39;split&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([14, 78, 11,  9, 10, 10, 25, 43, 12,  9, 22, 36, 17, 49, 10, 17,  8,
       17, 24, 10, 30, 72, 39, 27, 13, 12, 25, 76, 49,  4], dtype=int32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 重要度　gain</span>
<span class="n">model</span><span class="o">.</span><span class="n">feature_importance</span><span class="p">(</span><span class="n">importance_type</span><span class="o">=</span><span class="s1">&#39;gain&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1.46665834e+00, 8.09516829e+01, 1.27327641e+00, 1.00653108e+01,
       1.01958670e+01, 4.55437377e-03, 8.28053906e+00, 8.90929255e+02,
       1.40457279e-04, 7.74426483e-02, 2.17797562e+01, 4.34140159e+00,
       9.57603275e+00, 4.71241752e+01, 7.34331360e+00, 1.77204293e+01,
       2.32711246e+00, 1.69506009e+00, 1.37307583e+01, 4.97734045e-01,
       1.31161104e+02, 1.38687325e+02, 1.64405305e+02, 2.98311714e+02,
       9.33543229e+00, 5.97189658e-02, 7.30525910e+01, 8.74733118e+02,
       1.69859772e+01, 8.85340598e-02])
</pre></div>
</div>
</div>
</div>
</section>
<section id="id5">
<h2>欠損の扱い<a class="headerlink" href="#id5" title="Link to this heading">#</a></h2>
<p>LightGBMにおける欠損の扱いに関して、こちらの記事で表で簡潔にまとめられていてわかりやすかったです。<br />
<a class="reference external" href="https://nigimitama.hatenablog.jp/entry/2020/09/28/000000">https://nigimitama.hatenablog.jp/entry/2020/09/28/000000</a><br />
記事中にも書いてある通り、学習時に欠損がなかった場合の数値特徴量は、<br />
予測時は0で置換されてしまうので特に注意が必要です。</p>
</section>
<section id="id6">
<h2>再現性の確保<a class="headerlink" href="#id6" title="Link to this heading">#</a></h2>
<p>LightGBMの学習結果の再現性を確保するには、
次のパラメータを設定すると良さそうです。</p>
<ul class="simple">
<li><p>seed：任意の整数</p></li>
<li><p>deterministic：True</p></li>
<li><p>force_col_wiseまたはforce_row_wise：True</p></li>
</ul>
<p>なお、deterministicをTrueにすると訓練が遅くなるなど、<br />
注意事項が↓に記載されています</p>
<p><a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/Parameters.html">https://lightgbm.readthedocs.io/en/latest/Parameters.html</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">lightgbm</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lgb</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.datasets</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_x</span><span class="p">,</span> <span class="n">val_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">val_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dtrain</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">train_y</span><span class="p">)</span>
<span class="n">dval</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">val_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">val_y</span><span class="p">)</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;objective&quot;</span><span class="p">:</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span>
    <span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="s2">&quot;binary_logloss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;verbosity&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>

    <span class="s2">&quot;seed&quot;</span><span class="p">:</span> <span class="mi">42</span><span class="p">,</span>
    <span class="s2">&quot;deterministic&quot;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span>
    <span class="s2">&quot;force_col_wise&quot;</span><span class="p">:</span><span class="kc">True</span>
<span class="p">}</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="n">dtrain</span><span class="p">,</span>
    <span class="n">valid_sets</span><span class="o">=</span><span class="p">[</span><span class="n">dtrain</span><span class="p">,</span> <span class="n">dval</span><span class="p">],</span>
    <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">lgb</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">lgb</span><span class="o">.</span><span class="n">log_evaluation</span><span class="p">(</span><span class="mi">100</span><span class="p">)],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	training&#39;s binary_logloss: 0.000462198	valid_1&#39;s binary_logloss: 0.12469
Early stopping, best iteration is:
[41]	training&#39;s binary_logloss: 0.02643	valid_1&#39;s binary_logloss: 0.103654
</pre></div>
</div>
</div>
</div>
</section>
<section id="id7">
<h2>バージョンによる違い<a class="headerlink" href="#id7" title="Link to this heading">#</a></h2>
<p>LightGBMは昔のバージョンと最近のバージョンで、<br />
指定方法が変わっているオプションがあります。</p>
<p>昔は各オプションで設定していたものが、<br />
callbackで指定するようになりました。<br />
・early_stopping_rounds → early_stopping<br />
・evals_result → record_evaluation<br />
・verbose_eval → log_evaluation</p>
<p>v3.2.1：オプションで指定していたころのバージョン<br />
<a class="reference external" href="https://lightgbm.readthedocs.io/en/v3.2.1/pythonapi/lightgbm.train.html">https://lightgbm.readthedocs.io/en/v3.2.1/pythonapi/lightgbm.train.html</a></p>
<p>v4.5.0：コールバックで指定しているバージョン<br />
<a class="reference external" href="https://lightgbm.readthedocs.io/en/v4.5.0/pythonapi/lightgbm.train.html">https://lightgbm.readthedocs.io/en/v4.5.0/pythonapi/lightgbm.train.html</a></p>
<p>こちらの記事で簡潔に解説されていてとても参考になりました。<br />
<a class="reference external" href="https://zenn.dev/local/articles/e2e6de3959e96d">https://zenn.dev/local/articles/e2e6de3959e96d</a></p>
<p>こちらの記事も参考になりました。<br />
<a class="reference external" href="https://qiita.com/c60evaporator/items/2b7a2820d575e212bcf4">https://qiita.com/c60evaporator/items/2b7a2820d575e212bcf4</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># # 古いバージョン</span>
<span class="c1"># !pip install lightgbm==3.2.1</span>
<span class="c1"># # 実行後にカーネル再起動の必要あり</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># # 古いバージョンのやり方</span>
<span class="c1"># import lightgbm as lgb</span>
<span class="c1"># import sklearn.datasets</span>
<span class="c1"># from sklearn.model_selection import train_test_split</span>

<span class="c1"># data, target = sklearn.datasets.load_breast_cancer(return_X_y=True,as_frame=True)</span>
<span class="c1"># train_x, val_x, train_y, val_y = train_test_split(data, target, test_size=0.25,random_state=42)</span>
<span class="c1"># dtrain = lgb.Dataset(train_x, label=train_y)</span>
<span class="c1"># dval = lgb.Dataset(val_x, label=val_y)</span>

<span class="c1"># params = {</span>
<span class="c1">#     &quot;objective&quot;: &quot;binary&quot;,</span>
<span class="c1">#     &quot;metric&quot;: &quot;binary_logloss&quot;,</span>
<span class="c1">#     &quot;verbosity&quot;: -1,</span>
<span class="c1"># }</span>
<span class="c1"># evaluations_result={}  # 評価結果を入れる辞書</span>
<span class="c1"># booster = lgb.train(</span>
<span class="c1">#     params,</span>
<span class="c1">#     dtrain,</span>
<span class="c1">#     valid_sets=[dtrain, dval],</span>
<span class="c1">#     num_boost_round=1000,</span>
<span class="c1">#     early_stopping_rounds=100,</span>
<span class="c1">#     evals_result=evaluations_result,</span>
<span class="c1">#     verbose_eval=100</span>
<span class="c1"># )</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 新しいバージョン</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">lightgbm</span><span class="o">==</span><span class="mf">4.5.0</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: lightgbm==4.5.0 in /usr/local/lib/python3.11/dist-packages (4.5.0)
Requirement already satisfied: numpy&gt;=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm==4.5.0) (1.26.4)
Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm==4.5.0) (1.13.1)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 新しいバージョンのやり方</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">lightgbm</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lgb</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.datasets</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_x</span><span class="p">,</span> <span class="n">val_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">val_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dtrain</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">train_y</span><span class="p">)</span>
<span class="n">dval</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">val_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">val_y</span><span class="p">)</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;objective&quot;</span><span class="p">:</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span>
    <span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="s2">&quot;binary_logloss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;verbosity&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">evaluations_result</span><span class="o">=</span><span class="p">{}</span>  <span class="c1"># 評価結果を入れる辞書</span>
<span class="n">booster</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="n">dtrain</span><span class="p">,</span>
    <span class="n">valid_sets</span><span class="o">=</span><span class="p">[</span><span class="n">dtrain</span><span class="p">,</span> <span class="n">dval</span><span class="p">],</span>
    <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span>
        <span class="n">lgb</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
        <span class="n">lgb</span><span class="o">.</span><span class="n">log_evaluation</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
        <span class="n">lgb</span><span class="o">.</span><span class="n">record_evaluation</span><span class="p">(</span><span class="n">evaluations_result</span><span class="p">)</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	training&#39;s binary_logloss: 0.000462198	valid_1&#39;s binary_logloss: 0.12469
Early stopping, best iteration is:
[41]	training&#39;s binary_logloss: 0.02643	valid_1&#39;s binary_logloss: 0.103654
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id8">
<h1>可視化などの便利関数<a class="headerlink" href="#id8" title="Link to this heading">#</a></h1>
<section id="plot-metric">
<h2>plot_metric：評価値の推移の可視化<a class="headerlink" href="#plot-metric" title="Link to this heading">#</a></h2>
<p>lightgbmでtrainのときに、callbacksにrecord_evaluationを使うと、各iterationの評価結果を取得することができます。<br />
また、plot_metricを使うことで簡単に可視化できます。</p>
<p>record_evaluation<br />
<a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.record_evaluation.html">https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.record_evaluation.html</a></p>
<p>plot_metric<br />
<a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.plot_metric.html">https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.plot_metric.html</a></p>
<section id="train-api">
<h3>train APIの場合<a class="headerlink" href="#train-api" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">lightgbm</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lgb</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.datasets</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_x</span><span class="p">,</span> <span class="n">val_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">val_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dtrain</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">train_y</span><span class="p">)</span>
<span class="n">dval</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">val_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">val_y</span><span class="p">)</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;objective&quot;</span><span class="p">:</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span>
    <span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="s2">&quot;binary_logloss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;verbosity&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">evaluations_result</span><span class="o">=</span><span class="p">{}</span> <span class="c1"># 評価結果を入れる辞書</span>
<span class="n">booster</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="n">dtrain</span><span class="p">,</span>
    <span class="n">valid_sets</span><span class="o">=</span><span class="p">[</span><span class="n">dtrain</span><span class="p">,</span> <span class="n">dval</span><span class="p">],</span>
    <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span>
        <span class="n">lgb</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
        <span class="n">lgb</span><span class="o">.</span><span class="n">record_evaluation</span><span class="p">(</span><span class="n">evaluations_result</span><span class="p">)</span> <span class="c1"># 評価結果を辞書に入れる</span>
    <span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
Early stopping, best iteration is:
[41]	training&#39;s binary_logloss: 0.02643	valid_1&#39;s binary_logloss: 0.103654
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lgb</span><span class="o">.</span><span class="n">plot_metric</span><span class="p">(</span><span class="n">evaluations_result</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/b0f5cd8bcc881fc07f96ef1be7dfefa763fbd961cf43344491d32081e32e4878.png" src="../_images/b0f5cd8bcc881fc07f96ef1be7dfefa763fbd961cf43344491d32081e32e4878.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 評価指標を複数指定した場合</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;objective&quot;</span><span class="p">:</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span>
    <span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;binary_logloss&quot;</span><span class="p">,</span> <span class="s2">&quot;auc&quot;</span><span class="p">],</span> <span class="c1"># リストで評価指標を複数指定</span>
    <span class="s2">&quot;verbosity&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">evaluations_result</span><span class="o">=</span><span class="p">{}</span> <span class="c1"># 評価結果を入れる辞書</span>
<span class="n">booster</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="n">dtrain</span><span class="p">,</span>
    <span class="n">valid_sets</span><span class="o">=</span><span class="p">[</span><span class="n">dtrain</span><span class="p">,</span> <span class="n">dval</span><span class="p">],</span>
    <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span>
        <span class="n">lgb</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
        <span class="n">lgb</span><span class="o">.</span><span class="n">record_evaluation</span><span class="p">(</span><span class="n">evaluations_result</span><span class="p">)</span> <span class="c1"># 評価結果を辞書に入れる</span>
    <span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
Early stopping, best iteration is:
[41]	training&#39;s binary_logloss: 0.02643	training&#39;s auc: 1	valid_1&#39;s binary_logloss: 0.103654	valid_1&#39;s auc: 0.994174
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># metricオプションで可視化する指標を指定</span>
<span class="n">lgb</span><span class="o">.</span><span class="n">plot_metric</span><span class="p">(</span><span class="n">evaluations_result</span><span class="p">,</span><span class="n">metric</span><span class="o">=</span><span class="s1">&#39;binary_logloss&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Axes: title={&#39;center&#39;: &#39;Metric during training&#39;}, xlabel=&#39;Iterations&#39;, ylabel=&#39;binary_logloss&#39;&gt;
</pre></div>
</div>
<img alt="../_images/62ea70ce86a408fabdfba94d4c232d463c153d756e5677ff53b999d4544980e5.png" src="../_images/62ea70ce86a408fabdfba94d4c232d463c153d756e5677ff53b999d4544980e5.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># metricオプションで可視化する指標を指定</span>
<span class="n">lgb</span><span class="o">.</span><span class="n">plot_metric</span><span class="p">(</span><span class="n">evaluations_result</span><span class="p">,</span><span class="n">metric</span><span class="o">=</span><span class="s1">&#39;auc&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Axes: title={&#39;center&#39;: &#39;Metric during training&#39;}, xlabel=&#39;Iterations&#39;, ylabel=&#39;auc&#39;&gt;
</pre></div>
</div>
<img alt="../_images/7932aa3bc2af756a8349b632d4b8b055d337fc2ba870eee381d39e115a189eab.png" src="../_images/7932aa3bc2af756a8349b632d4b8b055d337fc2ba870eee381d39e115a189eab.png" />
</div>
</div>
</section>
<section id="sklearn-api">
<h3>sklearn APIの場合<a class="headerlink" href="#sklearn-api" title="Link to this heading">#</a></h3>
<p>sklearn APIでモデルを作成した場合は、plot_metricにモデルを入れると可視化されます。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># sklearn APIで使う場合</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">lightgbm</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lgb</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.datasets</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_x</span><span class="p">,</span> <span class="n">val_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">val_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">LGBMClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span><span class="n">verbosity</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span>
    <span class="n">eval_set</span><span class="o">=</span><span class="p">[(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">),(</span><span class="n">val_x</span><span class="p">,</span> <span class="n">val_y</span><span class="p">)],</span>
    <span class="n">eval_metric</span><span class="o">=</span><span class="s2">&quot;binary_logloss&quot;</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">lgb</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="mi">100</span><span class="p">)]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
Early stopping, best iteration is:
[41]	training&#39;s binary_logloss: 0.02643	valid_1&#39;s binary_logloss: 0.103654
</pre></div>
</div>
<div class="output text_html"><style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: #000;
  --sklearn-color-text-muted: #666;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: flex;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
  align-items: start;
  justify-content: space-between;
  gap: 0.5em;
}

#sk-container-id-1 label.sk-toggleable__label .caption {
  font-size: 0.6rem;
  font-weight: lighter;
  color: var(--sklearn-color-text-muted);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 0.5em;
  text-align: center;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LGBMClassifier(n_estimators=1000, verbosity=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>LGBMClassifier</div></div><div><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></div></label><div class="sk-toggleable__content fitted"><pre>LGBMClassifier(n_estimators=1000, verbosity=-1)</pre></div> </div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># sklearn APIの場合、モデルを入力すれば可視化される</span>
<span class="n">lgb</span><span class="o">.</span><span class="n">plot_metric</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Axes: title={&#39;center&#39;: &#39;Metric during training&#39;}, xlabel=&#39;Iterations&#39;, ylabel=&#39;binary_logloss&#39;&gt;
</pre></div>
</div>
<img alt="../_images/62ea70ce86a408fabdfba94d4c232d463c153d756e5677ff53b999d4544980e5.png" src="../_images/62ea70ce86a408fabdfba94d4c232d463c153d756e5677ff53b999d4544980e5.png" />
</div>
</div>
</section>
</section>
<section id="plot-importance">
<h2>plot_importance：重要度の可視化<a class="headerlink" href="#plot-importance" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">lightgbm</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lgb</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.datasets</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_x</span><span class="p">,</span> <span class="n">val_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">val_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dtrain</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">train_y</span><span class="p">)</span>
<span class="n">dval</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">val_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">val_y</span><span class="p">)</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;objective&quot;</span><span class="p">:</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span>
    <span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="s2">&quot;binary_logloss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;verbosity&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="n">dtrain</span><span class="p">,</span>
    <span class="n">valid_sets</span><span class="o">=</span><span class="p">[</span><span class="n">dtrain</span><span class="p">,</span> <span class="n">dval</span><span class="p">],</span>
    <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">lgb</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">lgb</span><span class="o">.</span><span class="n">log_evaluation</span><span class="p">(</span><span class="mi">100</span><span class="p">)],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	training&#39;s binary_logloss: 0.000462198	valid_1&#39;s binary_logloss: 0.12469
Early stopping, best iteration is:
[41]	training&#39;s binary_logloss: 0.02643	valid_1&#39;s binary_logloss: 0.103654
</pre></div>
</div>
</div>
</div>
<p>lightgbmのplot_importanceメソッドを使うと
重要度を可視化することができて便利です。</p>
<p>公式ドキュメント
<a class="reference external" href="https://lightgbm.readthedocs.io/en/stable/pythonapi/lightgbm.plot_importance.html">https://lightgbm.readthedocs.io/en/stable/pythonapi/lightgbm.plot_importance.html</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lgb</span><span class="o">.</span><span class="n">plot_importance</span><span class="p">(</span><span class="n">booster</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/146ace5d44c6265bb08df980e3ae664f9a3d5da921b6cb84e76e98a72578ee7e.png" src="../_images/146ace5d44c6265bb08df980e3ae664f9a3d5da921b6cb84e76e98a72578ee7e.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lgb</span><span class="o">.</span><span class="n">plot_importance</span><span class="p">(</span><span class="n">booster</span><span class="p">,</span><span class="n">importance_type</span><span class="o">=</span><span class="s1">&#39;gain&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a4bdc34f3f5c4cf625a064b394485fd225f31c46ae8654f703b29b418b9f9d31.png" src="../_images/a4bdc34f3f5c4cf625a064b394485fd225f31c46ae8654f703b29b418b9f9d31.png" />
</div>
</div>
</section>
<section id="plot-split-value-histogram">
<h2>plot_split_value_histogram：分岐に使われる値の可視化<a class="headerlink" href="#plot-split-value-histogram" title="Link to this heading">#</a></h2>
<p>lightgbmのplot_split_value_histogramを使うと、指定した特徴量がどの値で分岐に使われているか、<br />
その出現頻度を可視化することができます。</p>
<p>モデルを解釈するのに役立つかもしれません。</p>
<p><a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.plot_split_value_histogram.html">https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.plot_split_value_histogram.html</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">lightgbm</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lgb</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.datasets</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_x</span><span class="p">,</span> <span class="n">val_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">val_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dtrain</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">train_y</span><span class="p">)</span>
<span class="n">dval</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">val_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">val_y</span><span class="p">)</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;objective&quot;</span><span class="p">:</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span>
    <span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="s2">&quot;binary_logloss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;verbosity&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>

    <span class="s2">&quot;seed&quot;</span><span class="p">:</span> <span class="mi">42</span><span class="p">,</span>
    <span class="s2">&quot;deterministic&quot;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span>
    <span class="s2">&quot;force_col_wise&quot;</span><span class="p">:</span><span class="kc">True</span>
<span class="p">}</span>
<span class="n">booster</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="n">dtrain</span><span class="p">,</span>
    <span class="n">valid_sets</span><span class="o">=</span><span class="p">[</span><span class="n">dtrain</span><span class="p">,</span> <span class="n">dval</span><span class="p">],</span>
    <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">lgb</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">lgb</span><span class="o">.</span><span class="n">log_evaluation</span><span class="p">(</span><span class="mi">100</span><span class="p">)],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	training&#39;s binary_logloss: 0.000462198	valid_1&#39;s binary_logloss: 0.12469
Early stopping, best iteration is:
[41]	training&#39;s binary_logloss: 0.02643	valid_1&#39;s binary_logloss: 0.103654
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lgb</span><span class="o">.</span><span class="n">plot_split_value_histogram</span><span class="p">(</span><span class="n">booster</span><span class="p">,</span><span class="n">feature</span><span class="o">=</span><span class="s1">&#39;mean_texture&#39;</span><span class="p">);</span> <span class="c1"># featureに確認したい列名(or番号)を指定</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/c372feca4b5477dce74c6d15a3163f0f2bac4b921131613c524f403326f32ba2.png" src="../_images/c372feca4b5477dce74c6d15a3163f0f2bac4b921131613c524f403326f32ba2.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 分岐に使われている値が妥当そうか確認してみる</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;mean texture&#39;</span><span class="p">],</span><span class="n">hue</span><span class="o">=</span><span class="n">target</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/b394ca89ae93d4451970b1904480aa68097eb6a7055bb5c54c1e23529a7949da.png" src="../_images/b394ca89ae93d4451970b1904480aa68097eb6a7055bb5c54c1e23529a7949da.png" />
</div>
</div>
<section id="get-split-value-histogram">
<h3>get_split_value_histogram：分岐に使われる値の取得<a class="headerlink" href="#get-split-value-histogram" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html#lightgbm.Booster.get_split_value_histogram">https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html#lightgbm.Booster.get_split_value_histogram</a></p>
<p>get_split_value_histogramでグラフではなく値で取得できます</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sp_val</span> <span class="o">=</span> <span class="n">booster</span><span class="o">.</span><span class="n">get_split_value_histogram</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="s1">&#39;mean_texture&#39;</span><span class="p">)</span>
<span class="n">sp_val</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([ 1,  2,  1,  0,  1,  5,  3,  1,  4,  0,  6,  0,  6,  5,  1,  4,  1,
         2,  1,  2,  0,  3, 12,  1,  1,  4,  1,  5,  1,  0,  0,  0,  0,  0,
         0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2]),
 array([15.105     , 15.38902174, 15.67304348, 15.95706522, 16.24108696,
        16.5251087 , 16.80913043, 17.09315217, 17.37717391, 17.66119565,
        17.94521739, 18.22923913, 18.51326087, 18.79728261, 19.08130435,
        19.36532609, 19.64934783, 19.93336957, 20.2173913 , 20.50141304,
        20.78543478, 21.06945652, 21.35347826, 21.6375    , 21.92152174,
        22.20554348, 22.48956522, 22.77358696, 23.0576087 , 23.34163043,
        23.62565217, 23.90967391, 24.19369565, 24.47771739, 24.76173913,
        25.04576087, 25.32978261, 25.61380435, 25.89782609, 26.18184783,
        26.46586957, 26.7498913 , 27.03391304, 27.31793478, 27.60195652,
        27.88597826, 28.17      ]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">counts</span> <span class="o">=</span> <span class="n">sp_val</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">bins</span> <span class="o">=</span> <span class="n">sp_val</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">hist_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;bin_center&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">bins</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">bins</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s1">&#39;count&#39;</span><span class="p">:</span> <span class="n">counts</span>
<span class="p">})</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;bin_center&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;count&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">hist_df</span><span class="p">,</span> <span class="n">native_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Feature split value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Count&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/d88b2b7d56ca289ef110eb21e69fd2dc1360dfc0a8a07b72169f207183ce21bf.png" src="../_images/d88b2b7d56ca289ef110eb21e69fd2dc1360dfc0a8a07b72169f207183ce21bf.png" />
</div>
</div>
</section>
</section>
<section id="plot-tree">
<h2>plot_tree：決定木の可視化<a class="headerlink" href="#plot-tree" title="Link to this heading">#</a></h2>
<p>lightgbmのcreate_tree_digraphで、各決定木を可視化できます。
これもモデルの解釈に役立ちそうです。
<a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.create_tree_digraph.html">https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.create_tree_digraph.html</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">lightgbm</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lgb</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.datasets</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_x</span><span class="p">,</span> <span class="n">val_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">val_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dtrain</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">train_y</span><span class="p">)</span>
<span class="n">dval</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">val_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">val_y</span><span class="p">)</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;objective&quot;</span><span class="p">:</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span>
    <span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="s2">&quot;binary_logloss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;verbosity&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">booster</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="n">dtrain</span><span class="p">,</span>
    <span class="n">valid_sets</span><span class="o">=</span><span class="p">[</span><span class="n">dtrain</span><span class="p">,</span> <span class="n">dval</span><span class="p">],</span>
    <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">lgb</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="mi">100</span><span class="p">)]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
Early stopping, best iteration is:
[41]	training&#39;s binary_logloss: 0.02643	valid_1&#39;s binary_logloss: 0.103654
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lgb</span><span class="o">.</span><span class="n">create_tree_digraph</span><span class="p">(</span><span class="n">booster</span><span class="p">,</span><span class="n">tree_index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">show_info</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;data_percentage&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/fb6aa86c10e334d06bcf92557e913abcb3fa58f749da44add1fffa625ec4ec1b.svg" src="../_images/fb6aa86c10e334d06bcf92557e913abcb3fa58f749da44add1fffa625ec4ec1b.svg" />
</div>
</div>
<p>なお、lightgbmの木の可視化には、
plot_treeという関数もありますが、<br />
これはcreate_tree_digraphの結果をpng画像に変換してmatplotlibで表示しているだけのようです。</p>
<p>直接Jupyterにレンダリングした方がきれいに表示できるのでcreate_tree_digraph使う方がおすすめです。
<a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.plot_tree.html">https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.plot_tree.html</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lgb</span><span class="o">.</span><span class="n">plot_tree</span><span class="p">(</span><span class="n">booster</span><span class="p">,</span><span class="n">tree_index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">show_info</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;data_percentage&quot;</span><span class="p">],</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">19</span><span class="p">,</span><span class="mi">5</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f0a64d825b24cbd2cbee3b72835ee3c8658e0ed1627a2601a259d0a4f806e097.png" src="../_images/f0a64d825b24cbd2cbee3b72835ee3c8658e0ed1627a2601a259d0a4f806e097.png" />
</div>
</div>
<section id="trees-to-dataframe">
<h3>trees_to_dataframe：決定木をテーブル形式で取得<a class="headerlink" href="#trees-to-dataframe" title="Link to this heading">#</a></h3>
<p>lightgbm.Boosterのtrees_to_dataframe()メソッドを使うと、木構造をpandasのDataFrameで返してくれます。<br />
これもモデルの解釈に役立てられそうです。</p>
<p><a class="reference external" href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html#lightgbm.Booster.trees_to_dataframe">https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html#lightgbm.Booster.trees_to_dataframe</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">booster</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="n">dtrain</span><span class="p">,</span>
    <span class="n">valid_sets</span><span class="o">=</span><span class="p">[</span><span class="n">dtrain</span><span class="p">,</span> <span class="n">dval</span><span class="p">],</span>
    <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">lgb</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="mi">100</span><span class="p">)]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
Early stopping, best iteration is:
[41]	training&#39;s binary_logloss: 0.02643	valid_1&#39;s binary_logloss: 0.103654
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">booster</span><span class="o">.</span><span class="n">trees_to_dataframe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
  <div id="df-671573c1-705d-4643-88f3-601ceef79bce" class="colab-df-container">
    <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>tree_index</th>
      <th>node_depth</th>
      <th>node_index</th>
      <th>left_child</th>
      <th>right_child</th>
      <th>parent_index</th>
      <th>split_feature</th>
      <th>split_gain</th>
      <th>threshold</th>
      <th>decision_type</th>
      <th>missing_direction</th>
      <th>missing_type</th>
      <th>value</th>
      <th>weight</th>
      <th>count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>1</td>
      <td>0-S0</td>
      <td>0-S2</td>
      <td>0-S1</td>
      <td>None</td>
      <td>mean_concave_points</td>
      <td>2.930880e+02</td>
      <td>0.050125</td>
      <td>&lt;=</td>
      <td>left</td>
      <td>None</td>
      <td>0.528392</td>
      <td>0.000000</td>
      <td>426</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>2</td>
      <td>0-S2</td>
      <td>0-S4</td>
      <td>0-L3</td>
      <td>0-S0</td>
      <td>worst_radius</td>
      <td>1.988610e+01</td>
      <td>16.590000</td>
      <td>&lt;=</td>
      <td>left</td>
      <td>None</td>
      <td>0.661568</td>
      <td>62.066100</td>
      <td>266</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>3</td>
      <td>0-S4</td>
      <td>0-S5</td>
      <td>0-L5</td>
      <td>0-S2</td>
      <td>area_error</td>
      <td>1.391370e+00</td>
      <td>30.185000</td>
      <td>&lt;=</td>
      <td>left</td>
      <td>None</td>
      <td>0.678565</td>
      <td>56.932800</td>
      <td>244</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>4</td>
      <td>0-S5</td>
      <td>0-S6</td>
      <td>0-L6</td>
      <td>0-S4</td>
      <td>worst_texture</td>
      <td>4.942130e-01</td>
      <td>29.990000</td>
      <td>&lt;=</td>
      <td>left</td>
      <td>None</td>
      <td>0.683486</td>
      <td>51.799500</td>
      <td>222</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>5</td>
      <td>0-S6</td>
      <td>0-L0</td>
      <td>0-L7</td>
      <td>0-S5</td>
      <td>mean_radius</td>
      <td>1.421090e-14</td>
      <td>13.720000</td>
      <td>&lt;=</td>
      <td>left</td>
      <td>None</td>
      <td>0.687347</td>
      <td>44.799600</td>
      <td>192</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1592</th>
      <td>40</td>
      <td>8</td>
      <td>40-S19</td>
      <td>40-L16</td>
      <td>40-S23</td>
      <td>40-S15</td>
      <td>worst_perimeter</td>
      <td>3.027200e-06</td>
      <td>139.850000</td>
      <td>&lt;=</td>
      <td>left</td>
      <td>None</td>
      <td>-0.101307</td>
      <td>0.891797</td>
      <td>74</td>
    </tr>
    <tr>
      <th>1593</th>
      <td>40</td>
      <td>9</td>
      <td>40-L16</td>
      <td>None</td>
      <td>None</td>
      <td>40-S19</td>
      <td>None</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td>-0.101585</td>
      <td>0.272532</td>
      <td>20</td>
    </tr>
    <tr>
      <th>1594</th>
      <td>40</td>
      <td>9</td>
      <td>40-S23</td>
      <td>40-L20</td>
      <td>40-L24</td>
      <td>40-S19</td>
      <td>mean_texture</td>
      <td>1.628320e-07</td>
      <td>22.030000</td>
      <td>&lt;=</td>
      <td>left</td>
      <td>None</td>
      <td>-0.101185</td>
      <td>0.619266</td>
      <td>54</td>
    </tr>
    <tr>
      <th>1595</th>
      <td>40</td>
      <td>10</td>
      <td>40-L20</td>
      <td>None</td>
      <td>None</td>
      <td>40-S23</td>
      <td>None</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td>-0.101128</td>
      <td>0.275086</td>
      <td>25</td>
    </tr>
    <tr>
      <th>1596</th>
      <td>40</td>
      <td>10</td>
      <td>40-L24</td>
      <td>None</td>
      <td>None</td>
      <td>40-S23</td>
      <td>None</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td>-0.101231</td>
      <td>0.344179</td>
      <td>29</td>
    </tr>
  </tbody>
</table>
<p>1597 rows × 15 columns</p>
</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-671573c1-705d-4643-88f3-601ceef79bce')"
            title="Convert this dataframe to an interactive table."
            style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewBox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"/>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-671573c1-705d-4643-88f3-601ceef79bce button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-671573c1-705d-4643-88f3-601ceef79bce');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-dd208c11-1eae-474e-81fd-8f13c0b721f0">
  <button class="colab-df-quickchart" onclick="quickchart('df-dd208c11-1eae-474e-81fd-8f13c0b721f0')"
            title="Suggest charts"
            style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
     width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"/>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-dd208c11-1eae-474e-81fd-8f13c0b721f0 button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>

    </div>
  </div>
</div></div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="optuna">
<h1>Optuna：ハイパーパラメータ自動チューニング<a class="headerlink" href="#optuna" title="Link to this heading">#</a></h1>
<section id="lightgbmtuner">
<h2>LightGBMTuner<a class="headerlink" href="#lightgbmtuner" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://optuna.readthedocs.io/en/v2.0.0/reference/generated/optuna.integration.lightgbm.LightGBMTuner.html">https://optuna.readthedocs.io/en/v2.0.0/reference/generated/optuna.integration.lightgbm.LightGBMTuner.html</a></p>
<p>LightGBMTunerを使うとoptunaを使って自動パラメータチューニングができます。</p>
<ul class="simple">
<li><p>runで学習とパラーメタチューニング開始</p>
<ul>
<li><p>得られるオブジェクトから取得できるもの</p>
<ul>
<li><p>best_params：チューニングの結果得られた最良のパラメータ</p></li>
<li><p>get_best_booster()：そのパラメータで学習されたモデル</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">optuna.integration.lightgbm</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lgb</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.datasets</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">optuna</span>
<span class="n">optuna</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_x</span><span class="p">,</span> <span class="n">val_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">val_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dtrain</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">train_y</span><span class="p">)</span>
<span class="n">dval</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">val_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">val_y</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;objective&quot;</span><span class="p">:</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span>
    <span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="s2">&quot;binary_logloss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;verbosity&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;boosting_type&quot;</span><span class="p">:</span> <span class="s2">&quot;gbdt&quot;</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">tuner</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">LightGBMTuner</span><span class="p">(</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="n">dtrain</span><span class="p">,</span>
    <span class="n">valid_sets</span><span class="o">=</span><span class="p">[</span><span class="n">dtrain</span><span class="p">,</span> <span class="n">dval</span><span class="p">],</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">lgb</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">lgb</span><span class="o">.</span><span class="n">log_evaluation</span><span class="p">(</span><span class="mi">100</span><span class="p">)],</span>
    <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">run</span><span class="p">()</span> <span class="c1"># 学習とパラーメタチューニング開始</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:58:58,718] A new study created in memory with name: no-name-fdab25e0-8dec-404b-b012-bed0f6f78036
[I 2025-02-15 05:58:58,901] Trial 0 finished with value: 0.10365368587870742 and parameters: {&#39;feature_fraction&#39;: 1.0}. Best is trial 0 with value: 0.10365368587870742.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000462198	valid_1&#39;s binary_logloss: 0.12469
Early stopping, best iteration is:
[41]	valid_0&#39;s binary_logloss: 0.02643	valid_1&#39;s binary_logloss: 0.103654
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:58:59,038] Trial 1 finished with value: 0.09967463948028768 and parameters: {&#39;feature_fraction&#39;: 0.6}. Best is trial 1 with value: 0.09967463948028768.
[I 2025-02-15 05:58:59,154] Trial 2 finished with value: 0.09106794311735114 and parameters: {&#39;feature_fraction&#39;: 0.5}. Best is trial 2 with value: 0.09106794311735114.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000652176	valid_1&#39;s binary_logloss: 0.107014
Early stopping, best iteration is:
[77]	valid_0&#39;s binary_logloss: 0.00291134	valid_1&#39;s binary_logloss: 0.0996746
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00067437	valid_1&#39;s binary_logloss: 0.105895
Early stopping, best iteration is:
[51]	valid_0&#39;s binary_logloss: 0.0151281	valid_1&#39;s binary_logloss: 0.0910679
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:58:59,296] Trial 3 finished with value: 0.09888564736737716 and parameters: {&#39;feature_fraction&#39;: 0.7}. Best is trial 2 with value: 0.09106794311735114.
[I 2025-02-15 05:58:59,412] Trial 4 finished with value: 0.06920513067690687 and parameters: {&#39;feature_fraction&#39;: 0.4}. Best is trial 4 with value: 0.06920513067690687.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000571291	valid_1&#39;s binary_logloss: 0.102289
Early stopping, best iteration is:
[46]	valid_0&#39;s binary_logloss: 0.0201278	valid_1&#39;s binary_logloss: 0.0988856
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000807354	valid_1&#39;s binary_logloss: 0.0851023
Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.00369537	valid_1&#39;s binary_logloss: 0.0692051
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:58:59,605] Trial 5 finished with value: 0.10139190628425744 and parameters: {&#39;feature_fraction&#39;: 0.8999999999999999}. Best is trial 4 with value: 0.06920513067690687.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000478885	valid_1&#39;s binary_logloss: 0.117873
Early stopping, best iteration is:
[56]	valid_0&#39;s binary_logloss: 0.00950971	valid_1&#39;s binary_logloss: 0.101392
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:58:59,813] Trial 6 finished with value: 0.10371303990875513 and parameters: {&#39;feature_fraction&#39;: 0.8}. Best is trial 4 with value: 0.06920513067690687.
[I 2025-02-15 05:58:59,908] Trial 7 finished with value: 0.07147650172497075 and parameters: {&#39;num_leaves&#39;: 15}. Best is trial 7 with value: 0.07147650172497075.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000552204	valid_1&#39;s binary_logloss: 0.117536
Early stopping, best iteration is:
[77]	valid_0&#39;s binary_logloss: 0.00252485	valid_1&#39;s binary_logloss: 0.103713
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000779091	valid_1&#39;s binary_logloss: 0.0841271
Early stopping, best iteration is:
[77]	valid_0&#39;s binary_logloss: 0.00336348	valid_1&#39;s binary_logloss: 0.0714765
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:00,037] Trial 8 finished with value: 0.06920513067690687 and parameters: {&#39;num_leaves&#39;: 95}. Best is trial 8 with value: 0.06920513067690687.
[I 2025-02-15 05:59:00,158] Trial 9 finished with value: 0.06920513067690687 and parameters: {&#39;num_leaves&#39;: 142}. Best is trial 8 with value: 0.06920513067690687.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000807354	valid_1&#39;s binary_logloss: 0.0851023
Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.00369537	valid_1&#39;s binary_logloss: 0.0692051
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000807354	valid_1&#39;s binary_logloss: 0.0851023
Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.00369537	valid_1&#39;s binary_logloss: 0.0692051
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:00,324] Trial 10 finished with value: 0.06920513067690687 and parameters: {&#39;num_leaves&#39;: 253}. Best is trial 8 with value: 0.06920513067690687.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000807354	valid_1&#39;s binary_logloss: 0.0851023
Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.00369537	valid_1&#39;s binary_logloss: 0.0692051
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000807354	valid_1&#39;s binary_logloss: 0.0851023
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:00,473] Trial 11 finished with value: 0.06920513067690687 and parameters: {&#39;num_leaves&#39;: 77}. Best is trial 8 with value: 0.06920513067690687.
[I 2025-02-15 05:59:00,628] Trial 12 finished with value: 0.06920513067690687 and parameters: {&#39;num_leaves&#39;: 149}. Best is trial 8 with value: 0.06920513067690687.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.00369537	valid_1&#39;s binary_logloss: 0.0692051
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000807354	valid_1&#39;s binary_logloss: 0.0851023
Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.00369537	valid_1&#39;s binary_logloss: 0.0692051
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:00,780] Trial 13 finished with value: 0.06920513067690687 and parameters: {&#39;num_leaves&#39;: 73}. Best is trial 8 with value: 0.06920513067690687.
[I 2025-02-15 05:59:00,920] Trial 14 finished with value: 0.06920513067690687 and parameters: {&#39;num_leaves&#39;: 213}. Best is trial 8 with value: 0.06920513067690687.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000807354	valid_1&#39;s binary_logloss: 0.0851023
Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.00369537	valid_1&#39;s binary_logloss: 0.0692051
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000807354	valid_1&#39;s binary_logloss: 0.0851023
Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.00369537	valid_1&#39;s binary_logloss: 0.0692051
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:01,045] Trial 15 finished with value: 0.07187212396056919 and parameters: {&#39;num_leaves&#39;: 2}. Best is trial 8 with value: 0.06920513067690687.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0625089	valid_1&#39;s binary_logloss: 0.0900694
[200]	valid_0&#39;s binary_logloss: 0.0282984	valid_1&#39;s binary_logloss: 0.0808062
[300]	valid_0&#39;s binary_logloss: 0.0148849	valid_1&#39;s binary_logloss: 0.0756999
[400]	valid_0&#39;s binary_logloss: 0.00834898	valid_1&#39;s binary_logloss: 0.0739707
[500]	valid_0&#39;s binary_logloss: 0.00483136	valid_1&#39;s binary_logloss: 0.0732238
[600]	valid_0&#39;s binary_logloss: 0.00284742	valid_1&#39;s binary_logloss: 0.0731345
Early stopping, best iteration is:
[588]	valid_0&#39;s binary_logloss: 0.00303278	valid_1&#39;s binary_logloss: 0.0718721
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000807354	valid_1&#39;s binary_logloss: 0.0851023
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:01,184] Trial 16 finished with value: 0.06920513067690687 and parameters: {&#39;num_leaves&#39;: 87}. Best is trial 8 with value: 0.06920513067690687.
[I 2025-02-15 05:59:01,332] Trial 17 finished with value: 0.06920513067690687 and parameters: {&#39;num_leaves&#39;: 188}. Best is trial 8 with value: 0.06920513067690687.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.00369537	valid_1&#39;s binary_logloss: 0.0692051
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000807354	valid_1&#39;s binary_logloss: 0.0851023
Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.00369537	valid_1&#39;s binary_logloss: 0.0692051
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:01,484] Trial 18 finished with value: 0.06920513067690687 and parameters: {&#39;num_leaves&#39;: 113}. Best is trial 8 with value: 0.06920513067690687.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000807354	valid_1&#39;s binary_logloss: 0.0851023
Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.00369537	valid_1&#39;s binary_logloss: 0.0692051
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000807354	valid_1&#39;s binary_logloss: 0.0851023
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:01,644] Trial 19 finished with value: 0.06920513067690687 and parameters: {&#39;num_leaves&#39;: 36}. Best is trial 8 with value: 0.06920513067690687.
[I 2025-02-15 05:59:01,793] Trial 20 finished with value: 0.06920513067690687 and parameters: {&#39;num_leaves&#39;: 185}. Best is trial 8 with value: 0.06920513067690687.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.00369537	valid_1&#39;s binary_logloss: 0.0692051
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000807354	valid_1&#39;s binary_logloss: 0.0851023
Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.00369537	valid_1&#39;s binary_logloss: 0.0692051
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:01,926] Trial 21 finished with value: 0.06920513067690687 and parameters: {&#39;num_leaves&#39;: 149}. Best is trial 8 with value: 0.06920513067690687.
[I 2025-02-15 05:59:02,050] Trial 22 finished with value: 0.06920513067690687 and parameters: {&#39;num_leaves&#39;: 123}. Best is trial 8 with value: 0.06920513067690687.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000807354	valid_1&#39;s binary_logloss: 0.0851023
Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.00369537	valid_1&#39;s binary_logloss: 0.0692051
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000807354	valid_1&#39;s binary_logloss: 0.0851023
Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.00369537	valid_1&#39;s binary_logloss: 0.0692051
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:02,176] Trial 23 finished with value: 0.06920513067690687 and parameters: {&#39;num_leaves&#39;: 45}. Best is trial 8 with value: 0.06920513067690687.
[I 2025-02-15 05:59:02,308] Trial 24 finished with value: 0.06920513067690687 and parameters: {&#39;num_leaves&#39;: 103}. Best is trial 8 with value: 0.06920513067690687.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000807354	valid_1&#39;s binary_logloss: 0.0851023
Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.00369537	valid_1&#39;s binary_logloss: 0.0692051
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000807354	valid_1&#39;s binary_logloss: 0.0851023
Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.00369537	valid_1&#39;s binary_logloss: 0.0692051
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:02,446] Trial 25 finished with value: 0.06920513067690687 and parameters: {&#39;num_leaves&#39;: 152}. Best is trial 8 with value: 0.06920513067690687.
[I 2025-02-15 05:59:02,581] Trial 26 finished with value: 0.06920513067690687 and parameters: {&#39;num_leaves&#39;: 185}. Best is trial 8 with value: 0.06920513067690687.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000807354	valid_1&#39;s binary_logloss: 0.0851023
Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.00369537	valid_1&#39;s binary_logloss: 0.0692051
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000807354	valid_1&#39;s binary_logloss: 0.0851023
Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.00369537	valid_1&#39;s binary_logloss: 0.0692051
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:02,736] Trial 27 finished with value: 0.08293050798886754 and parameters: {&#39;bagging_fraction&#39;: 0.9466000052668426, &#39;bagging_freq&#39;: 4}. Best is trial 27 with value: 0.08293050798886754.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000978548	valid_1&#39;s binary_logloss: 0.0977769
Early stopping, best iteration is:
[67]	valid_0&#39;s binary_logloss: 0.00701246	valid_1&#39;s binary_logloss: 0.0829305
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:02,845] Trial 28 finished with value: 0.06562459330508015 and parameters: {&#39;bagging_fraction&#39;: 0.4115530118805466, &#39;bagging_freq&#39;: 1}. Best is trial 28 with value: 0.06562459330508015.
[I 2025-02-15 05:59:02,934] Trial 29 finished with value: 0.06334541293594137 and parameters: {&#39;bagging_fraction&#39;: 0.44401657797907823, &#39;bagging_freq&#39;: 1}. Best is trial 29 with value: 0.06334541293594137.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.0085124	valid_1&#39;s binary_logloss: 0.0745819
Early stopping, best iteration is:
[67]	valid_0&#39;s binary_logloss: 0.0224376	valid_1&#39;s binary_logloss: 0.0656246
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0070821	valid_1&#39;s binary_logloss: 0.0748026
Early stopping, best iteration is:
[73]	valid_0&#39;s binary_logloss: 0.0164266	valid_1&#39;s binary_logloss: 0.0633454
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0108636	valid_1&#39;s binary_logloss: 0.0772223
Early stopping, best iteration is:
[81]	valid_0&#39;s binary_logloss: 0.0172478	valid_1&#39;s binary_logloss: 0.069969
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:03,026] Trial 30 finished with value: 0.06996895521694563 and parameters: {&#39;bagging_fraction&#39;: 0.4024652297677235, &#39;bagging_freq&#39;: 1}. Best is trial 29 with value: 0.06334541293594137.
[I 2025-02-15 05:59:03,107] Trial 31 finished with value: 0.0699595093688812 and parameters: {&#39;bagging_fraction&#39;: 0.4080992847803487, &#39;bagging_freq&#39;: 1}. Best is trial 29 with value: 0.06334541293594137.
[I 2025-02-15 05:59:03,230] Trial 32 finished with value: 0.06508512839450974 and parameters: {&#39;bagging_fraction&#39;: 0.5969969310208576, &#39;bagging_freq&#39;: 1}. Best is trial 29 with value: 0.06334541293594137.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00971937	valid_1&#39;s binary_logloss: 0.0790956
Early stopping, best iteration is:
[74]	valid_0&#39;s binary_logloss: 0.0191434	valid_1&#39;s binary_logloss: 0.0699595
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00391804	valid_1&#39;s binary_logloss: 0.0791862
Early stopping, best iteration is:
[77]	valid_0&#39;s binary_logloss: 0.00976353	valid_1&#39;s binary_logloss: 0.0650851
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:03,327] Trial 33 finished with value: 0.07004151589134114 and parameters: {&#39;bagging_fraction&#39;: 0.5586641670532112, &#39;bagging_freq&#39;: 1}. Best is trial 29 with value: 0.06334541293594137.
[I 2025-02-15 05:59:03,439] Trial 34 finished with value: 0.06697974834575879 and parameters: {&#39;bagging_fraction&#39;: 0.5911511158636431, &#39;bagging_freq&#39;: 3}. Best is trial 29 with value: 0.06334541293594137.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00449663	valid_1&#39;s binary_logloss: 0.0723407
Early stopping, best iteration is:
[74]	valid_0&#39;s binary_logloss: 0.0114918	valid_1&#39;s binary_logloss: 0.0700415
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00445793	valid_1&#39;s binary_logloss: 0.0787556
Early stopping, best iteration is:
[74]	valid_0&#39;s binary_logloss: 0.0108794	valid_1&#39;s binary_logloss: 0.0669797
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:03,548] Trial 35 finished with value: 0.06448310226113449 and parameters: {&#39;bagging_fraction&#39;: 0.5946680520998815, &#39;bagging_freq&#39;: 2}. Best is trial 29 with value: 0.06334541293594137.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00415632	valid_1&#39;s binary_logloss: 0.0744836
Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.0109418	valid_1&#39;s binary_logloss: 0.0644831
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00443925	valid_1&#39;s binary_logloss: 0.0831264
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:03,687] Trial 36 finished with value: 0.0746549964214986 and parameters: {&#39;bagging_fraction&#39;: 0.5854201979579668, &#39;bagging_freq&#39;: 1}. Best is trial 29 with value: 0.06334541293594137.
[I 2025-02-15 05:59:03,767] Trial 37 finished with value: 0.06905812204181577 and parameters: {&#39;feature_fraction&#39;: 0.44800000000000006}. Best is trial 37 with value: 0.06905812204181577.
[I 2025-02-15 05:59:03,870] Trial 38 finished with value: 0.05958496008536256 and parameters: {&#39;feature_fraction&#39;: 0.48000000000000004}. Best is trial 38 with value: 0.05958496008536256.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[92]	valid_0&#39;s binary_logloss: 0.00576207	valid_1&#39;s binary_logloss: 0.074655
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00713967	valid_1&#39;s binary_logloss: 0.0791223
Early stopping, best iteration is:
[74]	valid_0&#39;s binary_logloss: 0.0150155	valid_1&#39;s binary_logloss: 0.0690581
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.007347	valid_1&#39;s binary_logloss: 0.0725202
Early stopping, best iteration is:
[70]	valid_0&#39;s binary_logloss: 0.0178879	valid_1&#39;s binary_logloss: 0.059585
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:03,950] Trial 39 finished with value: 0.06334541293594137 and parameters: {&#39;feature_fraction&#39;: 0.41600000000000004}. Best is trial 38 with value: 0.05958496008536256.
[I 2025-02-15 05:59:04,076] Trial 40 finished with value: 0.0688541631785362 and parameters: {&#39;lambda_l1&#39;: 1.447122766936006e-07, &#39;lambda_l2&#39;: 7.308673475890728}. Best is trial 40 with value: 0.0688541631785362.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0070821	valid_1&#39;s binary_logloss: 0.0748026
Early stopping, best iteration is:
[73]	valid_0&#39;s binary_logloss: 0.0164266	valid_1&#39;s binary_logloss: 0.0633454
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0701899	valid_1&#39;s binary_logloss: 0.0797161
[200]	valid_0&#39;s binary_logloss: 0.0396628	valid_1&#39;s binary_logloss: 0.0696807
[300]	valid_0&#39;s binary_logloss: 0.0281844	valid_1&#39;s binary_logloss: 0.0708813
Early stopping, best iteration is:
[256]	valid_0&#39;s binary_logloss: 0.0324352	valid_1&#39;s binary_logloss: 0.0688542
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:04,247] Trial 41 finished with value: 0.13708822138388946 and parameters: {&#39;lambda_l1&#39;: 8.412011849264685, &#39;lambda_l2&#39;: 1.2821331669984234e-08}. Best is trial 40 with value: 0.0688541631785362.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.16807	valid_1&#39;s binary_logloss: 0.155943
[200]	valid_0&#39;s binary_logloss: 0.158511	valid_1&#39;s binary_logloss: 0.147239
[300]	valid_0&#39;s binary_logloss: 0.155596	valid_1&#39;s binary_logloss: 0.144992
[400]	valid_0&#39;s binary_logloss: 0.153268	valid_1&#39;s binary_logloss: 0.143069
[500]	valid_0&#39;s binary_logloss: 0.152957	valid_1&#39;s binary_logloss: 0.142746
[600]	valid_0&#39;s binary_logloss: 0.152235	valid_1&#39;s binary_logloss: 0.14195
[700]	valid_0&#39;s binary_logloss: 0.150852	valid_1&#39;s binary_logloss: 0.140534
[800]	valid_0&#39;s binary_logloss: 0.150272	valid_1&#39;s binary_logloss: 0.13962
[900]	valid_0&#39;s binary_logloss: 0.148568	valid_1&#39;s binary_logloss: 0.138366
[1000]	valid_0&#39;s binary_logloss: 0.147589	valid_1&#39;s binary_logloss: 0.137088
Did not meet early stopping. Best iteration is:
[959]	valid_0&#39;s binary_logloss: 0.147589	valid_1&#39;s binary_logloss: 0.137088
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00887784	valid_1&#39;s binary_logloss: 0.0751379
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:04,330] Trial 42 finished with value: 0.0630116572179865 and parameters: {&#39;lambda_l1&#39;: 0.039589397058568364, &#39;lambda_l2&#39;: 0.00017820861292860429}. Best is trial 42 with value: 0.0630116572179865.
[I 2025-02-15 05:59:04,403] Trial 43 finished with value: 0.06144227154578796 and parameters: {&#39;lambda_l1&#39;: 0.1198125345458145, &#39;lambda_l2&#39;: 0.00012436048442304043}. Best is trial 43 with value: 0.06144227154578796.
[I 2025-02-15 05:59:04,477] Trial 44 finished with value: 0.059413347281599214 and parameters: {&#39;lambda_l1&#39;: 0.2109570059230127, &#39;lambda_l2&#39;: 0.00013175792360715176}. Best is trial 44 with value: 0.059413347281599214.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[62]	valid_0&#39;s binary_logloss: 0.0265107	valid_1&#39;s binary_logloss: 0.0630117
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0119953	valid_1&#39;s binary_logloss: 0.0658587
Early stopping, best iteration is:
[80]	valid_0&#39;s binary_logloss: 0.0172555	valid_1&#39;s binary_logloss: 0.0614423
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0138065	valid_1&#39;s binary_logloss: 0.0623938
Early stopping, best iteration is:
[79]	valid_0&#39;s binary_logloss: 0.0200907	valid_1&#39;s binary_logloss: 0.0594133
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.012653	valid_1&#39;s binary_logloss: 0.0628913
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:04,551] Trial 45 finished with value: 0.0592187132986585 and parameters: {&#39;lambda_l1&#39;: 0.16059070851860735, &#39;lambda_l2&#39;: 0.0001356003818644778}. Best is trial 45 with value: 0.0592187132986585.
[I 2025-02-15 05:59:04,675] Trial 46 finished with value: 0.060326890777987634 and parameters: {&#39;lambda_l1&#39;: 0.18032819923304588, &#39;lambda_l2&#39;: 9.702588495524397e-05}. Best is trial 45 with value: 0.0592187132986585.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[67]	valid_0&#39;s binary_logloss: 0.0261397	valid_1&#39;s binary_logloss: 0.0592187
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0131755	valid_1&#39;s binary_logloss: 0.064121
Early stopping, best iteration is:
[67]	valid_0&#39;s binary_logloss: 0.0262198	valid_1&#39;s binary_logloss: 0.0603269
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00948923	valid_1&#39;s binary_logloss: 0.0735446
Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.0166132	valid_1&#39;s binary_logloss: 0.0650259
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:04,755] Trial 47 finished with value: 0.0650259023992499 and parameters: {&#39;lambda_l1&#39;: 0.05266224537770313, &#39;lambda_l2&#39;: 0.00014045362354536982}. Best is trial 45 with value: 0.0592187132986585.
[I 2025-02-15 05:59:04,841] Trial 48 finished with value: 0.059985041792655534 and parameters: {&#39;lambda_l1&#39;: 0.18310820271352382, &#39;lambda_l2&#39;: 0.00012374120115553173}. Best is trial 45 with value: 0.0592187132986585.
[I 2025-02-15 05:59:04,927] Trial 49 finished with value: 0.059549443157980136 and parameters: {&#39;lambda_l1&#39;: 0.21943496199448906, &#39;lambda_l2&#39;: 3.578535686978501e-05}. Best is trial 45 with value: 0.0592187132986585.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.01361	valid_1&#39;s binary_logloss: 0.0612333
Early stopping, best iteration is:
[94]	valid_0&#39;s binary_logloss: 0.0146559	valid_1&#39;s binary_logloss: 0.059985
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0147685	valid_1&#39;s binary_logloss: 0.0609903
[200]	valid_0&#39;s binary_logloss: 0.00564659	valid_1&#39;s binary_logloss: 0.0654045
Early stopping, best iteration is:
[108]	valid_0&#39;s binary_logloss: 0.0140314	valid_1&#39;s binary_logloss: 0.0595494
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0556324	valid_1&#39;s binary_logloss: 0.0749809
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:05,061] Trial 50 finished with value: 0.06894081708583287 and parameters: {&#39;lambda_l1&#39;: 1.8614034526343088, &#39;lambda_l2&#39;: 3.532510116422294e-06}. Best is trial 45 with value: 0.0592187132986585.
[I 2025-02-15 05:59:05,135] Trial 51 finished with value: 0.06155118657861261 and parameters: {&#39;lambda_l1&#39;: 0.03362493584385771, &#39;lambda_l2&#39;: 6.763998833265609e-05}. Best is trial 45 with value: 0.0592187132986585.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[200]	valid_0&#39;s binary_logloss: 0.0420463	valid_1&#39;s binary_logloss: 0.0714429
[300]	valid_0&#39;s binary_logloss: 0.0385697	valid_1&#39;s binary_logloss: 0.0703691
[400]	valid_0&#39;s binary_logloss: 0.0359561	valid_1&#39;s binary_logloss: 0.0695749
[500]	valid_0&#39;s binary_logloss: 0.0343988	valid_1&#39;s binary_logloss: 0.0690955
Early stopping, best iteration is:
[453]	valid_0&#39;s binary_logloss: 0.035156	valid_1&#39;s binary_logloss: 0.0689408
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00830483	valid_1&#39;s binary_logloss: 0.0730934
Early stopping, best iteration is:
[69]	valid_0&#39;s binary_logloss: 0.0202003	valid_1&#39;s binary_logloss: 0.0615512
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0134582	valid_1&#39;s binary_logloss: 0.0641667
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:05,216] Trial 52 finished with value: 0.05963034748618566 and parameters: {&#39;lambda_l1&#39;: 0.20878728818448092, &#39;lambda_l2&#39;: 1.1926428142229112e-05}. Best is trial 45 with value: 0.0592187132986585.
[I 2025-02-15 05:59:05,297] Trial 53 finished with value: 0.05813170323672657 and parameters: {&#39;lambda_l1&#39;: 0.003065494380245861, &#39;lambda_l2&#39;: 3.2302110585247302e-06}. Best is trial 53 with value: 0.05813170323672657.
[I 2025-02-15 05:59:05,378] Trial 54 finished with value: 0.06289965424835077 and parameters: {&#39;lambda_l1&#39;: 0.0007512937493856766, &#39;lambda_l2&#39;: 1.3220864374391769e-06}. Best is trial 53 with value: 0.05813170323672657.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[80]	valid_0&#39;s binary_logloss: 0.0193742	valid_1&#39;s binary_logloss: 0.0596303
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0074614	valid_1&#39;s binary_logloss: 0.0682387
Early stopping, best iteration is:
[73]	valid_0&#39;s binary_logloss: 0.0158239	valid_1&#39;s binary_logloss: 0.0581317
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00701729	valid_1&#39;s binary_logloss: 0.0760419
Early stopping, best iteration is:
[70]	valid_0&#39;s binary_logloss: 0.0177976	valid_1&#39;s binary_logloss: 0.0628997
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:05,467] Trial 55 finished with value: 0.057886627427962946 and parameters: {&#39;lambda_l1&#39;: 0.0022803207334591027, &#39;lambda_l2&#39;: 1.8170674173230776e-06}. Best is trial 55 with value: 0.057886627427962946.
[I 2025-02-15 05:59:05,556] Trial 56 finished with value: 0.06651071708225775 and parameters: {&#39;lambda_l1&#39;: 0.0006603893558603172, &#39;lambda_l2&#39;: 3.9282494862866335e-07}. Best is trial 55 with value: 0.057886627427962946.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.0072056	valid_1&#39;s binary_logloss: 0.0717895
Early stopping, best iteration is:
[70]	valid_0&#39;s binary_logloss: 0.0179364	valid_1&#39;s binary_logloss: 0.0578866
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00729543	valid_1&#39;s binary_logloss: 0.0767914
Early stopping, best iteration is:
[69]	valid_0&#39;s binary_logloss: 0.0188382	valid_1&#39;s binary_logloss: 0.0665107
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00738989	valid_1&#39;s binary_logloss: 0.0729916
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:05,675] Trial 57 finished with value: 0.06297722791807454 and parameters: {&#39;lambda_l1&#39;: 0.0025342361330628173, &#39;lambda_l2&#39;: 0.006684453892186231}. Best is trial 55 with value: 0.057886627427962946.
[I 2025-02-15 05:59:05,761] Trial 58 finished with value: 0.06232554647773599 and parameters: {&#39;lambda_l1&#39;: 2.418521448035019e-05, &#39;lambda_l2&#39;: 1.5338865540469682e-07}. Best is trial 55 with value: 0.057886627427962946.
[I 2025-02-15 05:59:05,844] Trial 59 finished with value: 0.0651672782777711 and parameters: {&#39;lambda_l1&#39;: 0.005210028404781963, &#39;lambda_l2&#39;: 0.0049135864872116766}. Best is trial 55 with value: 0.057886627427962946.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.0145483	valid_1&#39;s binary_logloss: 0.0629772
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00747662	valid_1&#39;s binary_logloss: 0.0713667
Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.014287	valid_1&#39;s binary_logloss: 0.0623255
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00714569	valid_1&#39;s binary_logloss: 0.0754344
Early stopping, best iteration is:
[70]	valid_0&#39;s binary_logloss: 0.0171736	valid_1&#39;s binary_logloss: 0.0651673
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:05,935] Trial 60 finished with value: 0.05945369029263588 and parameters: {&#39;min_child_samples&#39;: 50}. Best is trial 60 with value: 0.05945369029263588.
[I 2025-02-15 05:59:06,047] Trial 61 finished with value: 0.06957910737232839 and parameters: {&#39;min_child_samples&#39;: 100}. Best is trial 60 with value: 0.05945369029263588.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.034445	valid_1&#39;s binary_logloss: 0.06781
[200]	valid_0&#39;s binary_logloss: 0.00874239	valid_1&#39;s binary_logloss: 0.0697344
Early stopping, best iteration is:
[136]	valid_0&#39;s binary_logloss: 0.0209778	valid_1&#39;s binary_logloss: 0.0594537
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.274041	valid_1&#39;s binary_logloss: 0.246436
[200]	valid_0&#39;s binary_logloss: 0.181769	valid_1&#39;s binary_logloss: 0.145909
[300]	valid_0&#39;s binary_logloss: 0.143588	valid_1&#39;s binary_logloss: 0.10938
[400]	valid_0&#39;s binary_logloss: 0.126399	valid_1&#39;s binary_logloss: 0.0943751
[500]	valid_0&#39;s binary_logloss: 0.119895	valid_1&#39;s binary_logloss: 0.0869653
[600]	valid_0&#39;s binary_logloss: 0.112663	valid_1&#39;s binary_logloss: 0.0823176
[700]	valid_0&#39;s binary_logloss: 0.105845	valid_1&#39;s binary_logloss: 0.0768464
[800]	valid_0&#39;s binary_logloss: 0.100689	valid_1&#39;s binary_logloss: 0.0700075
[900]	valid_0&#39;s binary_logloss: 0.0951931	valid_1&#39;s binary_logloss: 0.074564
Early stopping, best iteration is:
[813]	valid_0&#39;s binary_logloss: 0.0987089	valid_1&#39;s binary_logloss: 0.0695791
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:06,147] Trial 62 finished with value: 0.05742180191431429 and parameters: {&#39;min_child_samples&#39;: 5}. Best is trial 62 with value: 0.05742180191431429.
[I 2025-02-15 05:59:06,231] Trial 63 finished with value: 0.06091019357315468 and parameters: {&#39;min_child_samples&#39;: 10}. Best is trial 62 with value: 0.05742180191431429.
[I 2025-02-15 05:59:06,293] Trial 64 finished with value: 0.07019491427484138 and parameters: {&#39;min_child_samples&#39;: 25}. Best is trial 62 with value: 0.05742180191431429.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.00284403	valid_1&#39;s binary_logloss: 0.0778417
Early stopping, best iteration is:
[73]	valid_0&#39;s binary_logloss: 0.00669731	valid_1&#39;s binary_logloss: 0.0574218
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00448297	valid_1&#39;s binary_logloss: 0.0734796
Early stopping, best iteration is:
[67]	valid_0&#39;s binary_logloss: 0.012099	valid_1&#39;s binary_logloss: 0.0609102
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00929928	valid_1&#39;s binary_logloss: 0.0746002
Early stopping, best iteration is:
[58]	valid_0&#39;s binary_logloss: 0.0336543	valid_1&#39;s binary_logloss: 0.0701949
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># パラメータ</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">best_params</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;objective&#39;: &#39;binary&#39;,
 &#39;metric&#39;: &#39;binary_logloss&#39;,
 &#39;verbosity&#39;: -1,
 &#39;boosting_type&#39;: &#39;gbdt&#39;,
 &#39;feature_pre_filter&#39;: False,
 &#39;lambda_l1&#39;: 0.0022803207334591027,
 &#39;lambda_l2&#39;: 1.8170674173230776e-06,
 &#39;num_leaves&#39;: 31,
 &#39;feature_fraction&#39;: 0.48000000000000004,
 &#39;bagging_fraction&#39;: 0.44401657797907823,
 &#39;bagging_freq&#39;: 1,
 &#39;min_child_samples&#39;: 5}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 最良のパラメタで学習されたモデルを取得</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tuner</span><span class="o">.</span><span class="n">get_best_booster</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">val_x</span><span class="p">)[:</span><span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([9.97661147e-01, 4.33064785e-04, 7.19724539e-04])
</pre></div>
</div>
</div>
</div>
</section>
<section id="lightgbmtunercv">
<h2>LightGBMTunerCV<a class="headerlink" href="#lightgbmtunercv" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://optuna.readthedocs.io/en/v2.0.0/reference/generated/optuna.integration.lightgbm.LightGBMTunerCV.html">https://optuna.readthedocs.io/en/v2.0.0/reference/generated/optuna.integration.lightgbm.LightGBMTunerCV.html</a></p>
<p>optuna.integrationのLightGBMTunerCVを使うと、交差検証してパラメータチューニングを行います。</p>
<p>チューニングしたパラメータが得られ、return_cvbooster=Trueとすれば、
そのパラメータで学習されたモデルを取得することもできます。</p>
<p>サンプルコード<br />
<a class="github reference external" href="https://github.com/optuna/optuna-examples/blob/main/lightgbm/lightgbm_tuner_cv.py">optuna/optuna-examples</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">optuna.integration.lightgbm</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lgb</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.datasets</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">KFold</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dtrain</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">target</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;objective&quot;</span><span class="p">:</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span>
    <span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="s2">&quot;binary_logloss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;verbosity&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;boosting_type&quot;</span><span class="p">:</span> <span class="s2">&quot;gbdt&quot;</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">tuner</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">LightGBMTunerCV</span><span class="p">(</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="n">dtrain</span><span class="p">,</span>
    <span class="n">folds</span><span class="o">=</span><span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">lgb</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">lgb</span><span class="o">.</span><span class="n">log_evaluation</span><span class="p">(</span><span class="mi">100</span><span class="p">)],</span>
    <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">return_cvbooster</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">tuner</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:06,362] A new study created in memory with name: no-name-e709a8d8-4ea0-471c-aa7e-913114525962
/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:86: UserWarning: The groups parameter is ignored by KFold
  warnings.warn(
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.13854 + 0.087254
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:09,025] Trial 0 finished with value: 0.12025281209292614 and parameters: {&#39;feature_fraction&#39;: 1.0}. Best is trial 0 with value: 0.12025281209292614.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[49]	cv_agg&#39;s valid binary_logloss: 0.120253 + 0.062858
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:09,412] Trial 1 finished with value: 0.11030669491510992 and parameters: {&#39;feature_fraction&#39;: 0.6}. Best is trial 1 with value: 0.11030669491510992.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.130926 + 0.0846482
Early stopping, best iteration is:
[50]	cv_agg&#39;s valid binary_logloss: 0.110307 + 0.0539841
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:09,796] Trial 2 finished with value: 0.10476251316660355 and parameters: {&#39;feature_fraction&#39;: 0.5}. Best is trial 2 with value: 0.10476251316660355.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.119285 + 0.0771093
Early stopping, best iteration is:
[56]	cv_agg&#39;s valid binary_logloss: 0.104763 + 0.0524322
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:10,283] Trial 3 finished with value: 0.1098395780651082 and parameters: {&#39;feature_fraction&#39;: 0.7}. Best is trial 2 with value: 0.10476251316660355.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.127943 + 0.0796452
Early stopping, best iteration is:
[61]	cv_agg&#39;s valid binary_logloss: 0.10984 + 0.0590742
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:10,636] Trial 4 finished with value: 0.10766184362100834 and parameters: {&#39;feature_fraction&#39;: 0.4}. Best is trial 2 with value: 0.10476251316660355.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.119372 + 0.0747948
Early stopping, best iteration is:
[61]	cv_agg&#39;s valid binary_logloss: 0.107662 + 0.0548315
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:11,193] Trial 5 finished with value: 0.11425826711453106 and parameters: {&#39;feature_fraction&#39;: 0.8999999999999999}. Best is trial 2 with value: 0.10476251316660355.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.129458 + 0.0820114
Early stopping, best iteration is:
[70]	cv_agg&#39;s valid binary_logloss: 0.114258 + 0.0664533
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:11,678] Trial 6 finished with value: 0.11421610134146287 and parameters: {&#39;feature_fraction&#39;: 0.8}. Best is trial 2 with value: 0.10476251316660355.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.136798 + 0.0943453
Early stopping, best iteration is:
[50]	cv_agg&#39;s valid binary_logloss: 0.114216 + 0.0589948
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:12,088] Trial 7 finished with value: 0.10476251316660355 and parameters: {&#39;num_leaves&#39;: 183}. Best is trial 7 with value: 0.10476251316660355.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.119285 + 0.0771093
Early stopping, best iteration is:
[56]	cv_agg&#39;s valid binary_logloss: 0.104763 + 0.0524322
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.119285 + 0.0771093
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:13,589] Trial 8 finished with value: 0.10476251316660355 and parameters: {&#39;num_leaves&#39;: 196}. Best is trial 7 with value: 0.10476251316660355.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[56]	cv_agg&#39;s valid binary_logloss: 0.104763 + 0.0524322
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.119285 + 0.0771093
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:22,651] Trial 9 finished with value: 0.10476251316660355 and parameters: {&#39;num_leaves&#39;: 74}. Best is trial 7 with value: 0.10476251316660355.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[56]	cv_agg&#39;s valid binary_logloss: 0.104763 + 0.0524322
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.103914 + 0.0587365
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:25,076] Trial 10 finished with value: 0.10275741775346992 and parameters: {&#39;num_leaves&#39;: 4}. Best is trial 10 with value: 0.10275741775346992.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[90]	cv_agg&#39;s valid binary_logloss: 0.102757 + 0.0559683
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.120071 + 0.0786307
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:27,380] Trial 11 finished with value: 0.10492248865621608 and parameters: {&#39;num_leaves&#39;: 14}. Best is trial 10 with value: 0.10275741775346992.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[56]	cv_agg&#39;s valid binary_logloss: 0.104922 + 0.0522726
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:29,127] Trial 12 finished with value: 0.10545535208001122 and parameters: {&#39;num_leaves&#39;: 5}. Best is trial 10 with value: 0.10275741775346992.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.112268 + 0.065008
Early stopping, best iteration is:
[74]	cv_agg&#39;s valid binary_logloss: 0.105455 + 0.052644
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:29,509] Trial 13 finished with value: 0.10476251316660355 and parameters: {&#39;num_leaves&#39;: 106}. Best is trial 10 with value: 0.10275741775346992.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.119285 + 0.0771093
Early stopping, best iteration is:
[56]	cv_agg&#39;s valid binary_logloss: 0.104763 + 0.0524322
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:29,965] Trial 14 finished with value: 0.10476251316660355 and parameters: {&#39;num_leaves&#39;: 245}. Best is trial 10 with value: 0.10275741775346992.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.119285 + 0.0771093
Early stopping, best iteration is:
[56]	cv_agg&#39;s valid binary_logloss: 0.104763 + 0.0524322
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:30,331] Trial 15 finished with value: 0.10476251316660355 and parameters: {&#39;num_leaves&#39;: 64}. Best is trial 10 with value: 0.10275741775346992.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.119285 + 0.0771093
Early stopping, best iteration is:
[56]	cv_agg&#39;s valid binary_logloss: 0.104763 + 0.0524322
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:30,753] Trial 16 finished with value: 0.10476251316660355 and parameters: {&#39;num_leaves&#39;: 147}. Best is trial 10 with value: 0.10275741775346992.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.119285 + 0.0771093
Early stopping, best iteration is:
[56]	cv_agg&#39;s valid binary_logloss: 0.104763 + 0.0524322
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:31,145] Trial 17 finished with value: 0.10476251316660355 and parameters: {&#39;num_leaves&#39;: 53}. Best is trial 10 with value: 0.10275741775346992.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.119285 + 0.0771093
Early stopping, best iteration is:
[56]	cv_agg&#39;s valid binary_logloss: 0.104763 + 0.0524322
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:31,522] Trial 18 finished with value: 0.10476251316660355 and parameters: {&#39;num_leaves&#39;: 118}. Best is trial 10 with value: 0.10275741775346992.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.119285 + 0.0771093
Early stopping, best iteration is:
[56]	cv_agg&#39;s valid binary_logloss: 0.104763 + 0.0524322
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.119285 + 0.0771093
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:32,053] Trial 19 finished with value: 0.10476251316660355 and parameters: {&#39;num_leaves&#39;: 38}. Best is trial 10 with value: 0.10275741775346992.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[56]	cv_agg&#39;s valid binary_logloss: 0.104763 + 0.0524322
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:34,295] Trial 20 finished with value: 0.10476251316660355 and parameters: {&#39;num_leaves&#39;: 92}. Best is trial 10 with value: 0.10275741775346992.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.119285 + 0.0771093
Early stopping, best iteration is:
[56]	cv_agg&#39;s valid binary_logloss: 0.104763 + 0.0524322
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:34,698] Trial 21 finished with value: 0.10476251316660355 and parameters: {&#39;num_leaves&#39;: 167}. Best is trial 10 with value: 0.10275741775346992.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.119285 + 0.0771093
Early stopping, best iteration is:
[56]	cv_agg&#39;s valid binary_logloss: 0.104763 + 0.0524322
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:35,084] Trial 22 finished with value: 0.10476251316660355 and parameters: {&#39;num_leaves&#39;: 206}. Best is trial 10 with value: 0.10275741775346992.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.119285 + 0.0771093
Early stopping, best iteration is:
[56]	cv_agg&#39;s valid binary_logloss: 0.104763 + 0.0524322
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:35,482] Trial 23 finished with value: 0.10476251316660355 and parameters: {&#39;num_leaves&#39;: 251}. Best is trial 10 with value: 0.10275741775346992.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.119285 + 0.0771093
Early stopping, best iteration is:
[56]	cv_agg&#39;s valid binary_logloss: 0.104763 + 0.0524322
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:35,868] Trial 24 finished with value: 0.10476251316660355 and parameters: {&#39;num_leaves&#39;: 171}. Best is trial 10 with value: 0.10275741775346992.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.119285 + 0.0771093
Early stopping, best iteration is:
[56]	cv_agg&#39;s valid binary_logloss: 0.104763 + 0.0524322
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:36,288] Trial 25 finished with value: 0.10476251316660355 and parameters: {&#39;num_leaves&#39;: 218}. Best is trial 10 with value: 0.10275741775346992.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.119285 + 0.0771093
Early stopping, best iteration is:
[56]	cv_agg&#39;s valid binary_logloss: 0.104763 + 0.0524322
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:36,667] Trial 26 finished with value: 0.10476251316660355 and parameters: {&#39;num_leaves&#39;: 143}. Best is trial 10 with value: 0.10275741775346992.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.119285 + 0.0771093
Early stopping, best iteration is:
[56]	cv_agg&#39;s valid binary_logloss: 0.104763 + 0.0524322
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:36,840] Trial 27 finished with value: 0.10547334719326203 and parameters: {&#39;bagging_fraction&#39;: 0.9861269136389046, &#39;bagging_freq&#39;: 5}. Best is trial 27 with value: 0.10547334719326203.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.106444 + 0.056662
Early stopping, best iteration is:
[85]	cv_agg&#39;s valid binary_logloss: 0.105473 + 0.052022
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.110281 + 0.0639007
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:36,996] Trial 28 finished with value: 0.09949385247279692 and parameters: {&#39;bagging_fraction&#39;: 0.43168597422618016, &#39;bagging_freq&#39;: 1}. Best is trial 28 with value: 0.09949385247279692.
[I 2025-02-15 05:59:37,140] Trial 29 finished with value: 0.10026973511599781 and parameters: {&#39;bagging_fraction&#39;: 0.4475831279089103, &#39;bagging_freq&#39;: 1}. Best is trial 28 with value: 0.09949385247279692.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[65]	cv_agg&#39;s valid binary_logloss: 0.0994939 + 0.0491166
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.109033 + 0.0631426
Early stopping, best iteration is:
[67]	cv_agg&#39;s valid binary_logloss: 0.10027 + 0.0526146
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:37,311] Trial 30 finished with value: 0.09731020182862894 and parameters: {&#39;bagging_fraction&#39;: 0.4223424703035075, &#39;bagging_freq&#39;: 1}. Best is trial 30 with value: 0.09731020182862894.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.107724 + 0.062801
Early stopping, best iteration is:
[71]	cv_agg&#39;s valid binary_logloss: 0.0973102 + 0.0559897
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.10375 + 0.0596543
Early stopping, best iteration is:
[68]	cv_agg&#39;s valid binary_logloss: 0.0963618 + 0.0476843
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:37,464] Trial 31 finished with value: 0.09636177165048825 and parameters: {&#39;bagging_fraction&#39;: 0.4201624374210403, &#39;bagging_freq&#39;: 1}. Best is trial 31 with value: 0.09636177165048825.
[I 2025-02-15 05:59:37,661] Trial 32 finished with value: 0.09929801293409636 and parameters: {&#39;bagging_fraction&#39;: 0.40345288432977566, &#39;bagging_freq&#39;: 1}. Best is trial 31 with value: 0.09636177165048825.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.106853 + 0.0649858
Early stopping, best iteration is:
[81]	cv_agg&#39;s valid binary_logloss: 0.099298 + 0.0563374
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:37,824] Trial 33 finished with value: 0.0921060109950224 and parameters: {&#39;bagging_fraction&#39;: 0.40827307337534524, &#39;bagging_freq&#39;: 1}. Best is trial 33 with value: 0.0921060109950224.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.0992784 + 0.0584689
Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.092106 + 0.0508379
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.102888 + 0.065223
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:37,982] Trial 34 finished with value: 0.09477155728409115 and parameters: {&#39;bagging_fraction&#39;: 0.40259223692362256, &#39;bagging_freq&#39;: 1}. Best is trial 33 with value: 0.0921060109950224.
[I 2025-02-15 05:59:38,144] Trial 35 finished with value: 0.09424907778546976 and parameters: {&#39;bagging_fraction&#39;: 0.40212196913048553, &#39;bagging_freq&#39;: 1}. Best is trial 33 with value: 0.0921060109950224.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[81]	cv_agg&#39;s valid binary_logloss: 0.0947716 + 0.05475
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.102876 + 0.0645341
Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.0942491 + 0.0547959
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:38,320] Trial 36 finished with value: 0.10029115726750841 and parameters: {&#39;bagging_fraction&#39;: 0.5702613394198188, &#39;bagging_freq&#39;: 3}. Best is trial 33 with value: 0.0921060109950224.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.102083 + 0.054463
Early stopping, best iteration is:
[76]	cv_agg&#39;s valid binary_logloss: 0.100291 + 0.0478891
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.0976763 + 0.0464377
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:38,473] Trial 37 finished with value: 0.08905860425029266 and parameters: {&#39;feature_fraction&#39;: 0.5479999999999999}. Best is trial 37 with value: 0.08905860425029266.
[I 2025-02-15 05:59:38,636] Trial 38 finished with value: 0.09884179352786744 and parameters: {&#39;feature_fraction&#39;: 0.45199999999999996}. Best is trial 37 with value: 0.08905860425029266.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[77]	cv_agg&#39;s valid binary_logloss: 0.0890586 + 0.0376132
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.105316 + 0.0653112
Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.0988418 + 0.0572309
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:38,788] Trial 39 finished with value: 0.08792669132628889 and parameters: {&#39;feature_fraction&#39;: 0.58}. Best is trial 39 with value: 0.08792669132628889.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.094632 + 0.0505894
Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.0879267 + 0.0449807
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.113372 + 0.0704931
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:38,938] Trial 40 finished with value: 0.10614098074527807 and parameters: {&#39;feature_fraction&#39;: 0.42}. Best is trial 39 with value: 0.08792669132628889.
[I 2025-02-15 05:59:39,082] Trial 41 finished with value: 0.0921060109950224 and parameters: {&#39;feature_fraction&#39;: 0.484}. Best is trial 39 with value: 0.08792669132628889.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.106141 + 0.062441
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.0992784 + 0.0584689
Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.092106 + 0.0508379
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:39,251] Trial 42 finished with value: 0.0921060109950224 and parameters: {&#39;feature_fraction&#39;: 0.516}. Best is trial 39 with value: 0.08792669132628889.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.0992784 + 0.0584689
Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.092106 + 0.0508379
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.094543 + 0.0504647
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:39,436] Trial 43 finished with value: 0.08792594959708609 and parameters: {&#39;lambda_l1&#39;: 9.188450290527324e-05, &#39;lambda_l2&#39;: 2.0939674909709943e-06}. Best is trial 43 with value: 0.08792594959708609.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.0879259 + 0.0449782
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.0945453 + 0.0504678
Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.0879263 + 0.0449794
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:39,635] Trial 44 finished with value: 0.08792630172539022 and parameters: {&#39;lambda_l1&#39;: 4.690715813114397e-05, &#39;lambda_l2&#39;: 2.2710686656034647e-06}. Best is trial 43 with value: 0.08792594959708609.
[I 2025-02-15 05:59:39,804] Trial 45 finished with value: 0.0879259049350211 and parameters: {&#39;lambda_l1&#39;: 9.851825859031769e-05, &#39;lambda_l2&#39;: 1.2378842323833773e-06}. Best is trial 45 with value: 0.0879259049350211.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.0945427 + 0.0504643
Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.0879259 + 0.044978
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:40,005] Trial 46 finished with value: 0.08792615382797407 and parameters: {&#39;lambda_l1&#39;: 6.651416799627323e-05, &#39;lambda_l2&#39;: 1.5712526070485655e-06}. Best is trial 45 with value: 0.0879259049350211.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.0945443 + 0.0504665
Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.0879262 + 0.0449789
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.0945445 + 0.0504668
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:40,186] Trial 47 finished with value: 0.08792618696718629 and parameters: {&#39;lambda_l1&#39;: 6.282061659995161e-05, &#39;lambda_l2&#39;: 1.0863197187503637e-06}. Best is trial 45 with value: 0.0879259049350211.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.0879262 + 0.044979
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.0945454 + 0.0504678
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:40,396] Trial 48 finished with value: 0.08792631150293967 and parameters: {&#39;lambda_l1&#39;: 4.670867973114104e-05, &#39;lambda_l2&#39;: 1.3515988134139407e-06}. Best is trial 45 with value: 0.0879259049350211.
[I 2025-02-15 05:59:40,581] Trial 49 finished with value: 0.08792616505732576 and parameters: {&#39;lambda_l1&#39;: 6.514875073492065e-05, &#39;lambda_l2&#39;: 1.5022506720538548e-06}. Best is trial 45 with value: 0.0879259049350211.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.0879263 + 0.0449794
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.0945444 + 0.0504666
Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.0879262 + 0.0449789
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:40,784] Trial 50 finished with value: 0.08792573664322968 and parameters: {&#39;lambda_l1&#39;: 0.00012019608678099835, &#39;lambda_l2&#39;: 1.077668373288838e-06}. Best is trial 50 with value: 0.08792573664322968.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.0945415 + 0.0504629
Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.0879257 + 0.0449774
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:40,964] Trial 51 finished with value: 0.08792606648112823 and parameters: {&#39;lambda_l1&#39;: 7.800955209764467e-05, &#39;lambda_l2&#39;: 1.2197305494598312e-06}. Best is trial 50 with value: 0.08792573664322968.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.0945437 + 0.0504657
Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.0879261 + 0.0449786
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.0945444 + 0.0504666
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:41,147] Trial 52 finished with value: 0.08792617067338536 and parameters: {&#39;lambda_l1&#39;: 6.456083938098313e-05, &#39;lambda_l2&#39;: 1.3869946686587917e-06}. Best is trial 50 with value: 0.08792573664322968.
[I 2025-02-15 05:59:41,330] Trial 53 finished with value: 0.08792551557024338 and parameters: {&#39;lambda_l1&#39;: 0.00014812745882105877, &#39;lambda_l2&#39;: 1.573627411683062e-06}. Best is trial 53 with value: 0.08792551557024338.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.0879262 + 0.0449789
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.0945401 + 0.050461
Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.0879255 + 0.0449766
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:41,520] Trial 54 finished with value: 0.08844921663219812 and parameters: {&#39;lambda_l1&#39;: 0.0004423012908421534, &#39;lambda_l2&#39;: 8.697013721038643e-07}. Best is trial 53 with value: 0.08792551557024338.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.0947414 + 0.0507456
Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.0884492 + 0.0457079
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:41,726] Trial 55 finished with value: 0.08845077655508034 and parameters: {&#39;lambda_l1&#39;: 0.0010275754559255464, &#39;lambda_l2&#39;: 5.7969294334575586e-05}. Best is trial 53 with value: 0.08792551557024338.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.0947206 + 0.0507177
Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.0884508 + 0.0456995
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.0946325 + 0.05059
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:41,909] Trial 56 finished with value: 0.08792668696601436 and parameters: {&#39;lambda_l1&#39;: 4.935059452779334e-07, &#39;lambda_l2&#39;: 1.5351316740771947e-08}. Best is trial 53 with value: 0.08792551557024338.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.0879267 + 0.0449807
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.129164 + 0.0528151
[200]	cv_agg&#39;s valid binary_logloss: 0.109814 + 0.047984
[300]	cv_agg&#39;s valid binary_logloss: 0.104124 + 0.0488592
[400]	cv_agg&#39;s valid binary_logloss: 0.101518 + 0.0481638
[500]	cv_agg&#39;s valid binary_logloss: 0.100497 + 0.0478905
[600]	cv_agg&#39;s valid binary_logloss: 0.0993451 + 0.0489327
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:42,610] Trial 57 finished with value: 0.09708359239316618 and parameters: {&#39;lambda_l1&#39;: 1.202199136258063, &#39;lambda_l2&#39;: 4.666849390295913}. Best is trial 53 with value: 0.08792551557024338.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[700]	cv_agg&#39;s valid binary_logloss: 0.0984755 + 0.0477883
[800]	cv_agg&#39;s valid binary_logloss: 0.0975631 + 0.0478083
[900]	cv_agg&#39;s valid binary_logloss: 0.0973529 + 0.0474134
[1000]	cv_agg&#39;s valid binary_logloss: 0.0974729 + 0.0470605
Did not meet early stopping. Best iteration is:
[956]	cv_agg&#39;s valid binary_logloss: 0.0970836 + 0.0471049
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:42,792] Trial 58 finished with value: 0.09183193220004289 and parameters: {&#39;lambda_l1&#39;: 0.0019848554412857976, &#39;lambda_l2&#39;: 4.779439382142279e-08}. Best is trial 53 with value: 0.08792551557024338.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.0961152 + 0.0481044
Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.0918319 + 0.0439811
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:42,979] Trial 59 finished with value: 0.0879262083349487 and parameters: {&#39;lambda_l1&#39;: 6.056854821419909e-06, &#39;lambda_l2&#39;: 4.918619686438854e-05}. Best is trial 53 with value: 0.08792551557024338.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.0945447 + 0.050467
Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.0879262 + 0.0449793
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.0962332 + 0.0532131
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:43,168] Trial 60 finished with value: 0.08962132095104598 and parameters: {&#39;lambda_l1&#39;: 0.002818531457868026, &#39;lambda_l2&#39;: 1.1269688276779832e-05}. Best is trial 53 with value: 0.08792551557024338.
[I 2025-02-15 05:59:43,345] Trial 61 finished with value: 0.08792632889723946 and parameters: {&#39;lambda_l1&#39;: 4.56789895629217e-05, &#39;lambda_l2&#39;: 2.80152650911154e-07}. Best is trial 53 with value: 0.08792551557024338.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.0896213 + 0.044289
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.0945455 + 0.050468
Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.0879263 + 0.0449795
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:43,549] Trial 62 finished with value: 0.08792659589388463 and parameters: {&#39;lambda_l1&#39;: 6.252897837276702e-06, &#39;lambda_l2&#39;: 5.1973605787188255e-06}. Best is trial 53 with value: 0.08792551557024338.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.0945472 + 0.0504703
Early stopping, best iteration is:
[84]	cv_agg&#39;s valid binary_logloss: 0.0879266 + 0.0449804
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:43,768] Trial 63 finished with value: 0.10054768041484212 and parameters: {&#39;min_child_samples&#39;: 50}. Best is trial 63 with value: 0.10054768041484212.
[I 2025-02-15 05:59:43,823] Trial 64 finished with value: 0.6961322240168272 and parameters: {&#39;min_child_samples&#39;: 100}. Best is trial 63 with value: 0.10054768041484212.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.109009 + 0.0429978
[200]	cv_agg&#39;s valid binary_logloss: 0.110073 + 0.0636939
Early stopping, best iteration is:
[169]	cv_agg&#39;s valid binary_logloss: 0.100548 + 0.0460196
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.696132 + 0.0642465
Early stopping, best iteration is:
[1]	cv_agg&#39;s valid binary_logloss: 0.696132 + 0.0642465
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:44,018] Trial 65 finished with value: 0.09884987671239664 and parameters: {&#39;min_child_samples&#39;: 5}. Best is trial 65 with value: 0.09884987671239664.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	cv_agg&#39;s valid binary_logloss: 0.106093 + 0.0619588
Early stopping, best iteration is:
[78]	cv_agg&#39;s valid binary_logloss: 0.0988499 + 0.0562795
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.108597 + 0.0582791
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:44,501] Trial 66 finished with value: 0.10060546520292601 and parameters: {&#39;min_child_samples&#39;: 10}. Best is trial 65 with value: 0.09884987671239664.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[66]	cv_agg&#39;s valid binary_logloss: 0.100605 + 0.0487642
Training until validation scores don&#39;t improve for 100 rounds
[100]	cv_agg&#39;s valid binary_logloss: 0.0934211 + 0.0472404
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:46,075] Trial 67 finished with value: 0.08882248855998585 and parameters: {&#39;min_child_samples&#39;: 25}. Best is trial 67 with value: 0.08882248855998585.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[86]	cv_agg&#39;s valid binary_logloss: 0.0888225 + 0.039133
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tuner</span><span class="o">.</span><span class="n">best_params</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;objective&#39;: &#39;binary&#39;,
 &#39;metric&#39;: &#39;binary_logloss&#39;,
 &#39;verbosity&#39;: -1,
 &#39;boosting_type&#39;: &#39;gbdt&#39;,
 &#39;feature_pre_filter&#39;: False,
 &#39;lambda_l1&#39;: 0.00014812745882105877,
 &#39;lambda_l2&#39;: 1.573627411683062e-06,
 &#39;num_leaves&#39;: 4,
 &#39;feature_fraction&#39;: 0.58,
 &#39;bagging_fraction&#39;: 0.40827307337534524,
 &#39;bagging_freq&#39;: 1,
 &#39;min_child_samples&#39;: 20}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cvbooster</span> <span class="o">=</span> <span class="n">tuner</span><span class="o">.</span><span class="n">get_best_booster</span><span class="p">()</span> <span class="c1"># モデルを取得</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">predicted_data</span> <span class="o">=</span> <span class="n">cvbooster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">predicted_data</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> <span class="c1"># 各列が各foldの予測結果になるよう表示</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
  <div id="df-2ae704d7-9b0c-4a37-8159-e2d3af53754b" class="colab-df-container">
    <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.007600</td>
      <td>0.009454</td>
      <td>0.006949</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.001509</td>
      <td>0.007297</td>
      <td>0.008719</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.000140</td>
      <td>0.000349</td>
      <td>0.000438</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.212880</td>
      <td>0.064325</td>
      <td>0.047374</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.014974</td>
      <td>0.028381</td>
      <td>0.007978</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>564</th>
      <td>0.000249</td>
      <td>0.000180</td>
      <td>0.000264</td>
    </tr>
    <tr>
      <th>565</th>
      <td>0.000148</td>
      <td>0.000608</td>
      <td>0.000623</td>
    </tr>
    <tr>
      <th>566</th>
      <td>0.001730</td>
      <td>0.016042</td>
      <td>0.002984</td>
    </tr>
    <tr>
      <th>567</th>
      <td>0.000123</td>
      <td>0.000138</td>
      <td>0.000462</td>
    </tr>
    <tr>
      <th>568</th>
      <td>0.997655</td>
      <td>0.987911</td>
      <td>0.976019</td>
    </tr>
  </tbody>
</table>
<p>569 rows × 3 columns</p>
</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-2ae704d7-9b0c-4a37-8159-e2d3af53754b')"
            title="Convert this dataframe to an interactive table."
            style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewBox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"/>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-2ae704d7-9b0c-4a37-8159-e2d3af53754b button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-2ae704d7-9b0c-4a37-8159-e2d3af53754b');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-5b96bdc2-f80f-4f49-9105-7f164f82e5ba">
  <button class="colab-df-quickchart" onclick="quickchart('df-5b96bdc2-f80f-4f49-9105-7f164f82e5ba')"
            title="Suggest charts"
            style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
     width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"/>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-5b96bdc2-f80f-4f49-9105-7f164f82e5ba button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>

    </div>
  </div>
</div></div>
</div>
</section>
<section id="optuna-integration-lightgbm-tain">
<h2>optuna.integration.lightgbm.tain<a class="headerlink" href="#optuna-integration-lightgbm-tain" title="Link to this heading">#</a></h2>
<p>import optuna.integration.lightgbm as lgb<br />
上記のようにimportすれば、
lgb.trainでlightGBMのハイパーパラメータを自動チューニングしてくれます。<br />
（内部的にはLightGBMTunerクラスを呼び出しています）</p>
<p>サンプルコード<br />
<a class="github reference external" href="https://github.com/optuna/optuna-examples/blob/main/lightgbm/lightgbm_tuner_simple.py">optuna/optuna-examples</a></p>
<p>optuna-integrationリポジトリ<br />
<a class="github reference external" href="https://github.com/optuna/optuna-integration">optuna/optuna-integration</a></p>
<p>↓ドキュメントのsourceリンクを見るとどう呼び出されているかわかります。<br />
<a class="reference external" href="https://optuna.readthedocs.io/en/v3.0.1/reference/generated/optuna.integration.lightgbm.train.html#optuna.integration.lightgbm.train">https://optuna.readthedocs.io/en/v3.0.1/reference/generated/optuna.integration.lightgbm.train.html#optuna.integration.lightgbm.train</a></p>
<p>LightGBMTunerのドキュメント<br />
<a class="reference external" href="https://optuna.readthedocs.io/en/v3.0.1/reference/generated/optuna.integration.lightgbm.LightGBMTuner.html#optuna.integration.lightgbm.LightGBMTuner">https://optuna.readthedocs.io/en/v3.0.1/reference/generated/optuna.integration.lightgbm.LightGBMTuner.html#optuna.integration.lightgbm.LightGBMTuner</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">optuna.integration.lightgbm</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lgb</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">lightgbm</span><span class="w"> </span><span class="kn">import</span> <span class="n">early_stopping</span><span class="p">,</span><span class="n">log_evaluation</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.datasets</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_x</span><span class="p">,</span> <span class="n">val_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">val_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
<span class="n">dtrain</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">train_y</span><span class="p">)</span>
<span class="n">dval</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">val_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">val_y</span><span class="p">)</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;objective&quot;</span><span class="p">:</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span>
    <span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="s2">&quot;binary_logloss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;verbosity&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;boosting_type&quot;</span><span class="p">:</span> <span class="s2">&quot;gbdt&quot;</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="n">dtrain</span><span class="p">,</span>
    <span class="n">valid_sets</span><span class="o">=</span><span class="p">[</span><span class="n">dtrain</span><span class="p">,</span> <span class="n">dval</span><span class="p">],</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">early_stopping</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">log_evaluation</span><span class="p">(</span><span class="mi">100</span><span class="p">)],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 05:59:46,231] A new study created in memory with name: no-name-e89c1cc6-0798-48c1-97e9-68e08781b624
feature_fraction, val_score: inf:   0%|          | 0/7 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000436228	valid_1&#39;s binary_logloss: 0.156398
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction, val_score: 0.112962:  14%|#4        | 1/7 [00:01&lt;00:10,  1.70s/it][I 2025-02-15 05:59:47,946] Trial 0 finished with value: 0.1129622087308216 and parameters: {&#39;feature_fraction&#39;: 1.0}. Best is trial 0 with value: 0.1129622087308216.
feature_fraction, val_score: 0.112962:  14%|#4        | 1/7 [00:01&lt;00:10,  1.70s/it]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[44]	valid_0&#39;s binary_logloss: 0.0208577	valid_1&#39;s binary_logloss: 0.112962
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000531145	valid_1&#39;s binary_logloss: 0.143212
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction, val_score: 0.108024:  29%|##8       | 2/7 [00:03&lt;00:09,  1.89s/it][I 2025-02-15 05:59:49,968] Trial 1 finished with value: 0.10802357580752403 and parameters: {&#39;feature_fraction&#39;: 0.6}. Best is trial 1 with value: 0.10802357580752403.
feature_fraction, val_score: 0.108024:  29%|##8       | 2/7 [00:03&lt;00:09,  1.89s/it]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[52]	valid_0&#39;s binary_logloss: 0.0127929	valid_1&#39;s binary_logloss: 0.108024
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000629835	valid_1&#39;s binary_logloss: 0.146268
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction, val_score: 0.108024:  43%|####2     | 3/7 [00:05&lt;00:07,  1.90s/it][I 2025-02-15 05:59:51,890] Trial 2 finished with value: 0.11000961305219038 and parameters: {&#39;feature_fraction&#39;: 0.5}. Best is trial 1 with value: 0.10802357580752403.
feature_fraction, val_score: 0.108024:  43%|####2     | 3/7 [00:05&lt;00:07,  1.90s/it]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[48]	valid_0&#39;s binary_logloss: 0.0179141	valid_1&#39;s binary_logloss: 0.11001
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000570061	valid_1&#39;s binary_logloss: 0.140143
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction, val_score: 0.108024:  57%|#####7    | 4/7 [00:07&lt;00:05,  1.94s/it][I 2025-02-15 05:59:53,896] Trial 3 finished with value: 0.11122228771185085 and parameters: {&#39;feature_fraction&#39;: 0.7}. Best is trial 1 with value: 0.10802357580752403.
feature_fraction, val_score: 0.108024:  57%|#####7    | 4/7 [00:07&lt;00:05,  1.94s/it]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[52]	valid_0&#39;s binary_logloss: 0.0133681	valid_1&#39;s binary_logloss: 0.111222
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000760626	valid_1&#39;s binary_logloss: 0.133669
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction, val_score: 0.105307:  71%|#######1  | 5/7 [00:10&lt;00:04,  2.27s/it][I 2025-02-15 05:59:56,751] Trial 4 finished with value: 0.10530689023045 and parameters: {&#39;feature_fraction&#39;: 0.4}. Best is trial 4 with value: 0.10530689023045.
feature_fraction, val_score: 0.105307:  71%|#######1  | 5/7 [00:10&lt;00:04,  2.27s/it]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[54]	valid_0&#39;s binary_logloss: 0.0139989	valid_1&#39;s binary_logloss: 0.105307
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000453032	valid_1&#39;s binary_logloss: 0.144576
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction, val_score: 0.105307:  86%|########5 | 6/7 [00:14&lt;00:02,  2.94s/it][I 2025-02-15 06:00:00,997] Trial 5 finished with value: 0.10852574108167307 and parameters: {&#39;feature_fraction&#39;: 0.8999999999999999}. Best is trial 4 with value: 0.10530689023045.
feature_fraction, val_score: 0.105307:  86%|########5 | 6/7 [00:14&lt;00:02,  2.94s/it]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[51]	valid_0&#39;s binary_logloss: 0.0131653	valid_1&#39;s binary_logloss: 0.108526
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000482244	valid_1&#39;s binary_logloss: 0.147661
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction, val_score: 0.104860: 100%|##########| 7/7 [00:16&lt;00:00,  2.63s/it][I 2025-02-15 06:00:02,992] Trial 6 finished with value: 0.10486019776420186 and parameters: {&#39;feature_fraction&#39;: 0.8}. Best is trial 6 with value: 0.10486019776420186.
feature_fraction, val_score: 0.104860: 100%|##########| 7/7 [00:16&lt;00:00,  2.39s/it]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[45]	valid_0&#39;s binary_logloss: 0.0200125	valid_1&#39;s binary_logloss: 0.10486
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.104860:   0%|          | 0/20 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.104860:   5%|5         | 1/20 [00:01&lt;00:21,  1.15s/it][I 2025-02-15 06:00:04,159] Trial 7 finished with value: 0.10486019776420186 and parameters: {&#39;num_leaves&#39;: 39}. Best is trial 7 with value: 0.10486019776420186.
num_leaves, val_score: 0.104860:  10%|#         | 2/20 [00:01&lt;00:10,  1.77it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000482244	valid_1&#39;s binary_logloss: 0.147661
Early stopping, best iteration is:
[45]	valid_0&#39;s binary_logloss: 0.0200125	valid_1&#39;s binary_logloss: 0.10486
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000482244	valid_1&#39;s binary_logloss: 0.147661
Early stopping, best iteration is:
[45]	valid_0&#39;s binary_logloss: 0.0200125	valid_1&#39;s binary_logloss: 0.10486
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 06:00:04,313] Trial 8 finished with value: 0.10486019776420186 and parameters: {&#39;num_leaves&#39;: 58}. Best is trial 7 with value: 0.10486019776420186.
num_leaves, val_score: 0.104860:  15%|#5        | 3/20 [00:01&lt;00:06,  2.54it/s][I 2025-02-15 06:00:04,504] Trial 9 finished with value: 0.10486019776420186 and parameters: {&#39;num_leaves&#39;: 124}. Best is trial 7 with value: 0.10486019776420186.
num_leaves, val_score: 0.104860:  15%|#5        | 3/20 [00:01&lt;00:06,  2.54it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000482244	valid_1&#39;s binary_logloss: 0.147661
Early stopping, best iteration is:
[45]	valid_0&#39;s binary_logloss: 0.0200125	valid_1&#39;s binary_logloss: 0.10486
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.104860:  20%|##        | 4/20 [00:01&lt;00:05,  3.10it/s][I 2025-02-15 06:00:04,716] Trial 10 finished with value: 0.10486019776420186 and parameters: {&#39;num_leaves&#39;: 250}. Best is trial 7 with value: 0.10486019776420186.
num_leaves, val_score: 0.104860:  20%|##        | 4/20 [00:01&lt;00:05,  3.10it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000482244	valid_1&#39;s binary_logloss: 0.147661
Early stopping, best iteration is:
[45]	valid_0&#39;s binary_logloss: 0.0200125	valid_1&#39;s binary_logloss: 0.10486
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.100716:  20%|##        | 4/20 [00:01&lt;00:05,  3.10it/s][I 2025-02-15 06:00:04,802] Trial 11 finished with value: 0.10071556857656616 and parameters: {&#39;num_leaves&#39;: 5}. Best is trial 11 with value: 0.10071556857656616.
num_leaves, val_score: 0.100716:  30%|###       | 6/20 [00:01&lt;00:02,  4.95it/s][I 2025-02-15 06:00:04,906] Trial 12 finished with value: 0.10666158225548449 and parameters: {&#39;num_leaves&#39;: 10}. Best is trial 11 with value: 0.10071556857656616.
num_leaves, val_score: 0.100716:  30%|###       | 6/20 [00:01&lt;00:02,  4.95it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.00300222	valid_1&#39;s binary_logloss: 0.114171
Early stopping, best iteration is:
[68]	valid_0&#39;s binary_logloss: 0.0110816	valid_1&#39;s binary_logloss: 0.100716
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000503019	valid_1&#39;s binary_logloss: 0.146629
Early stopping, best iteration is:
[52]	valid_0&#39;s binary_logloss: 0.0128179	valid_1&#39;s binary_logloss: 0.106662
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.100716:  35%|###5      | 7/20 [00:02&lt;00:02,  5.17it/s][I 2025-02-15 06:00:05,078] Trial 13 finished with value: 0.10486019776420186 and parameters: {&#39;num_leaves&#39;: 163}. Best is trial 11 with value: 0.10071556857656616.
num_leaves, val_score: 0.100716:  35%|###5      | 7/20 [00:02&lt;00:02,  5.17it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000482244	valid_1&#39;s binary_logloss: 0.147661
Early stopping, best iteration is:
[45]	valid_0&#39;s binary_logloss: 0.0200125	valid_1&#39;s binary_logloss: 0.10486
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000482244	valid_1&#39;s binary_logloss: 0.147661
Early stopping, best iteration is:
[45]	valid_0&#39;s binary_logloss: 0.0200125	valid_1&#39;s binary_logloss: 0.10486
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.100716:  40%|####      | 8/20 [00:02&lt;00:02,  5.41it/s][I 2025-02-15 06:00:05,240] Trial 14 finished with value: 0.10486019776420186 and parameters: {&#39;num_leaves&#39;: 112}. Best is trial 11 with value: 0.10071556857656616.
num_leaves, val_score: 0.100716:  45%|####5     | 9/20 [00:02&lt;00:02,  5.39it/s][I 2025-02-15 06:00:05,427] Trial 15 finished with value: 0.10486019776420186 and parameters: {&#39;num_leaves&#39;: 202}. Best is trial 11 with value: 0.10071556857656616.
num_leaves, val_score: 0.100716:  45%|####5     | 9/20 [00:02&lt;00:02,  5.39it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000482244	valid_1&#39;s binary_logloss: 0.147661
Early stopping, best iteration is:
[45]	valid_0&#39;s binary_logloss: 0.0200125	valid_1&#39;s binary_logloss: 0.10486
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.100716:  50%|#####     | 10/20 [00:02&lt;00:01,  5.46it/s][I 2025-02-15 06:00:05,609] Trial 16 finished with value: 0.10486019776420186 and parameters: {&#39;num_leaves&#39;: 84}. Best is trial 11 with value: 0.10071556857656616.
num_leaves, val_score: 0.100716:  50%|#####     | 10/20 [00:02&lt;00:01,  5.46it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000482244	valid_1&#39;s binary_logloss: 0.147661
Early stopping, best iteration is:
[45]	valid_0&#39;s binary_logloss: 0.0200125	valid_1&#39;s binary_logloss: 0.10486
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000517665	valid_1&#39;s binary_logloss: 0.143266
Early stopping, best iteration is:
[46]	valid_0&#39;s binary_logloss: 0.0188081	valid_1&#39;s binary_logloss: 0.114948
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.100716:  55%|#####5    | 11/20 [00:02&lt;00:01,  5.77it/s][I 2025-02-15 06:00:05,755] Trial 17 finished with value: 0.11494766246565855 and parameters: {&#39;num_leaves&#39;: 9}. Best is trial 11 with value: 0.10071556857656616.
num_leaves, val_score: 0.100716:  60%|######    | 12/20 [00:02&lt;00:01,  5.79it/s][I 2025-02-15 06:00:05,927] Trial 18 finished with value: 0.10486019776420186 and parameters: {&#39;num_leaves&#39;: 160}. Best is trial 11 with value: 0.10071556857656616.
num_leaves, val_score: 0.100716:  60%|######    | 12/20 [00:02&lt;00:01,  5.79it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000482244	valid_1&#39;s binary_logloss: 0.147661
Early stopping, best iteration is:
[45]	valid_0&#39;s binary_logloss: 0.0200125	valid_1&#39;s binary_logloss: 0.10486
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.100716:  65%|######5   | 13/20 [00:03&lt;00:01,  5.85it/s][I 2025-02-15 06:00:06,093] Trial 19 finished with value: 0.10486019776420186 and parameters: {&#39;num_leaves&#39;: 78}. Best is trial 11 with value: 0.10071556857656616.
num_leaves, val_score: 0.100716:  65%|######5   | 13/20 [00:03&lt;00:01,  5.85it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000482244	valid_1&#39;s binary_logloss: 0.147661
Early stopping, best iteration is:
[45]	valid_0&#39;s binary_logloss: 0.0200125	valid_1&#39;s binary_logloss: 0.10486
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000482244	valid_1&#39;s binary_logloss: 0.147661
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.100716:  70%|#######   | 14/20 [00:03&lt;00:01,  5.78it/s][I 2025-02-15 06:00:06,271] Trial 20 finished with value: 0.10486019776420186 and parameters: {&#39;num_leaves&#39;: 210}. Best is trial 11 with value: 0.10071556857656616.
num_leaves, val_score: 0.100716:  75%|#######5  | 15/20 [00:03&lt;00:00,  5.68it/s][I 2025-02-15 06:00:06,454] Trial 21 finished with value: 0.10486019776420186 and parameters: {&#39;num_leaves&#39;: 39}. Best is trial 11 with value: 0.10071556857656616.
num_leaves, val_score: 0.100716:  75%|#######5  | 15/20 [00:03&lt;00:00,  5.68it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[45]	valid_0&#39;s binary_logloss: 0.0200125	valid_1&#39;s binary_logloss: 0.10486
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000482244	valid_1&#39;s binary_logloss: 0.147661
Early stopping, best iteration is:
[45]	valid_0&#39;s binary_logloss: 0.0200125	valid_1&#39;s binary_logloss: 0.10486
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.100716:  80%|########  | 16/20 [00:03&lt;00:00,  5.28it/s][I 2025-02-15 06:00:06,675] Trial 22 finished with value: 0.10486019776420186 and parameters: {&#39;num_leaves&#39;: 32}. Best is trial 11 with value: 0.10071556857656616.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000482244	valid_1&#39;s binary_logloss: 0.147661
Early stopping, best iteration is:
[45]	valid_0&#39;s binary_logloss: 0.0200125	valid_1&#39;s binary_logloss: 0.10486
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.100716:  85%|########5 | 17/20 [00:03&lt;00:00,  5.24it/s][I 2025-02-15 06:00:06,871] Trial 23 finished with value: 0.10486019776420186 and parameters: {&#39;num_leaves&#39;: 52}. Best is trial 11 with value: 0.10071556857656616.
num_leaves, val_score: 0.100716:  85%|########5 | 17/20 [00:03&lt;00:00,  5.24it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000482244	valid_1&#39;s binary_logloss: 0.147661
Early stopping, best iteration is:
[45]	valid_0&#39;s binary_logloss: 0.0200125	valid_1&#39;s binary_logloss: 0.10486
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.100716:  90%|######### | 18/20 [00:04&lt;00:00,  5.41it/s][I 2025-02-15 06:00:07,040] Trial 24 finished with value: 0.10486019776420186 and parameters: {&#39;num_leaves&#39;: 86}. Best is trial 11 with value: 0.10071556857656616.
num_leaves, val_score: 0.100716:  90%|######### | 18/20 [00:04&lt;00:00,  5.41it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000482244	valid_1&#39;s binary_logloss: 0.147661
Early stopping, best iteration is:
[45]	valid_0&#39;s binary_logloss: 0.0200125	valid_1&#39;s binary_logloss: 0.10486
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000482244	valid_1&#39;s binary_logloss: 0.147661
Early stopping, best iteration is:
[45]	valid_0&#39;s binary_logloss: 0.0200125	valid_1&#39;s binary_logloss: 0.10486
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.100716:  95%|#########5| 19/20 [00:04&lt;00:00,  5.59it/s][I 2025-02-15 06:00:07,205] Trial 25 finished with value: 0.10486020497827583 and parameters: {&#39;num_leaves&#39;: 26}. Best is trial 11 with value: 0.10071556857656616.
num_leaves, val_score: 0.100059:  95%|#########5| 19/20 [00:04&lt;00:00,  5.59it/s][I 2025-02-15 06:00:07,277] Trial 26 finished with value: 0.10005891718381331 and parameters: {&#39;num_leaves&#39;: 3}. Best is trial 26 with value: 0.10005891718381331.
num_leaves, val_score: 0.100059: 100%|##########| 20/20 [00:04&lt;00:00,  4.68it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.01886	valid_1&#39;s binary_logloss: 0.103625
[200]	valid_0&#39;s binary_logloss: 0.0032371	valid_1&#39;s binary_logloss: 0.112856
Early stopping, best iteration is:
[116]	valid_0&#39;s binary_logloss: 0.0138499	valid_1&#39;s binary_logloss: 0.100059
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bagging, val_score: 0.098820:   0%|          | 0/10 [00:00&lt;?, ?it/s][I 2025-02-15 06:00:07,388] Trial 27 finished with value: 0.09881989531115591 and parameters: {&#39;bagging_fraction&#39;: 0.8933219505082667, &#39;bagging_freq&#39;: 5}. Best is trial 27 with value: 0.09881989531115591.
bagging, val_score: 0.098820:  10%|#         | 1/10 [00:00&lt;00:00, 10.24it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0170863	valid_1&#39;s binary_logloss: 0.101584
[200]	valid_0&#39;s binary_logloss: 0.0027	valid_1&#39;s binary_logloss: 0.109988
Early stopping, best iteration is:
[145]	valid_0&#39;s binary_logloss: 0.00711128	valid_1&#39;s binary_logloss: 0.0988199
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bagging, val_score: 0.086170:  20%|##        | 2/10 [00:00&lt;00:00,  9.51it/s][I 2025-02-15 06:00:07,507] Trial 28 finished with value: 0.08616959230763528 and parameters: {&#39;bagging_fraction&#39;: 0.9083655341853746, &#39;bagging_freq&#39;: 5}. Best is trial 28 with value: 0.08616959230763528.
bagging, val_score: 0.086170:  20%|##        | 2/10 [00:00&lt;00:00,  9.51it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.0176205	valid_1&#39;s binary_logloss: 0.0925039
[200]	valid_0&#39;s binary_logloss: 0.00278101	valid_1&#39;s binary_logloss: 0.0977143
Early stopping, best iteration is:
[145]	valid_0&#39;s binary_logloss: 0.00729908	valid_1&#39;s binary_logloss: 0.0861696
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bagging, val_score: 0.086170:  30%|###       | 3/10 [00:00&lt;00:00,  9.08it/s][I 2025-02-15 06:00:07,626] Trial 29 finished with value: 0.09256929593405015 and parameters: {&#39;bagging_fraction&#39;: 0.9174700520488207, &#39;bagging_freq&#39;: 5}. Best is trial 28 with value: 0.08616959230763528.
bagging, val_score: 0.086170:  30%|###       | 3/10 [00:00&lt;00:00,  9.08it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.01772	valid_1&#39;s binary_logloss: 0.0969826
[200]	valid_0&#39;s binary_logloss: 0.00281986	valid_1&#39;s binary_logloss: 0.107568
Early stopping, best iteration is:
[143]	valid_0&#39;s binary_logloss: 0.00765958	valid_1&#39;s binary_logloss: 0.0925693
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0176341	valid_1&#39;s binary_logloss: 0.0916559
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bagging, val_score: 0.086170:  40%|####      | 4/10 [00:00&lt;00:00,  7.72it/s][I 2025-02-15 06:00:07,788] Trial 30 finished with value: 0.08918632589161905 and parameters: {&#39;bagging_fraction&#39;: 0.9253758571983607, &#39;bagging_freq&#39;: 5}. Best is trial 28 with value: 0.08616959230763528.
bagging, val_score: 0.086170:  40%|####      | 4/10 [00:00&lt;00:00,  7.72it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[200]	valid_0&#39;s binary_logloss: 0.00282485	valid_1&#39;s binary_logloss: 0.0988862
Early stopping, best iteration is:
[144]	valid_0&#39;s binary_logloss: 0.00754001	valid_1&#39;s binary_logloss: 0.0891863
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0177424	valid_1&#39;s binary_logloss: 0.0936884
[200]	valid_0&#39;s binary_logloss: 0.00284831	valid_1&#39;s binary_logloss: 0.10338
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bagging, val_score: 0.086170:  40%|####      | 4/10 [00:00&lt;00:00,  7.72it/s][I 2025-02-15 06:00:07,882] Trial 31 finished with value: 0.09256411006847287 and parameters: {&#39;bagging_fraction&#39;: 0.9219987028997414, &#39;bagging_freq&#39;: 5}. Best is trial 28 with value: 0.08616959230763528.
bagging, val_score: 0.086170:  50%|#####     | 5/10 [00:00&lt;00:00,  7.72it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[141]	valid_0&#39;s binary_logloss: 0.00811912	valid_1&#39;s binary_logloss: 0.0925641
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0176683	valid_1&#39;s binary_logloss: 0.0947002
Early stopping, best iteration is:
[99]	valid_0&#39;s binary_logloss: 0.0180229	valid_1&#39;s binary_logloss: 0.0933758
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bagging, val_score: 0.086170:  60%|######    | 6/10 [00:00&lt;00:00,  9.24it/s][I 2025-02-15 06:00:07,964] Trial 32 finished with value: 0.0933758090889258 and parameters: {&#39;bagging_fraction&#39;: 0.9396772640801001, &#39;bagging_freq&#39;: 5}. Best is trial 28 with value: 0.08616959230763528.
bagging, val_score: 0.086170:  60%|######    | 6/10 [00:00&lt;00:00,  9.24it/s][I 2025-02-15 06:00:08,046] Trial 33 finished with value: 0.09016252384933118 and parameters: {&#39;bagging_fraction&#39;: 0.9212305785852914, &#39;bagging_freq&#39;: 5}. Best is trial 28 with value: 0.08616959230763528.
bagging, val_score: 0.086170:  70%|#######   | 7/10 [00:00&lt;00:00,  9.24it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0178824	valid_1&#39;s binary_logloss: 0.0921873
Early stopping, best iteration is:
[92]	valid_0&#39;s binary_logloss: 0.0208804	valid_1&#39;s binary_logloss: 0.0901625
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bagging, val_score: 0.084350:  80%|########  | 8/10 [00:00&lt;00:00,  9.92it/s][I 2025-02-15 06:00:08,145] Trial 34 finished with value: 0.08435042633165667 and parameters: {&#39;bagging_fraction&#39;: 0.9140580186094145, &#39;bagging_freq&#39;: 5}. Best is trial 34 with value: 0.08435042633165667.
bagging, val_score: 0.084350:  80%|########  | 8/10 [00:00&lt;00:00,  9.92it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.0175414	valid_1&#39;s binary_logloss: 0.090751
[200]	valid_0&#39;s binary_logloss: 0.00273525	valid_1&#39;s binary_logloss: 0.0997948
Early stopping, best iteration is:
[145]	valid_0&#39;s binary_logloss: 0.00725553	valid_1&#39;s binary_logloss: 0.0843504
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bagging, val_score: 0.084350:  90%|######### | 9/10 [00:00&lt;00:00,  9.86it/s][I 2025-02-15 06:00:08,249] Trial 35 finished with value: 0.09262192834254106 and parameters: {&#39;bagging_fraction&#39;: 0.749986756411027, &#39;bagging_freq&#39;: 4}. Best is trial 34 with value: 0.08435042633165667.
bagging, val_score: 0.084350:  90%|######### | 9/10 [00:00&lt;00:00,  9.86it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.0178643	valid_1&#39;s binary_logloss: 0.105056
[200]	valid_0&#39;s binary_logloss: 0.00247507	valid_1&#39;s binary_logloss: 0.0966386
Early stopping, best iteration is:
[186]	valid_0&#39;s binary_logloss: 0.00317891	valid_1&#39;s binary_logloss: 0.0926219
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0227193	valid_1&#39;s binary_logloss: 0.0961411
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bagging, val_score: 0.084350:  90%|######### | 9/10 [00:01&lt;00:00,  9.86it/s][I 2025-02-15 06:00:08,326] Trial 36 finished with value: 0.09484688686064977 and parameters: {&#39;bagging_fraction&#39;: 0.4466904711182903, &#39;bagging_freq&#39;: 7}. Best is trial 34 with value: 0.08435042633165667.
bagging, val_score: 0.084350: 100%|##########| 10/10 [00:01&lt;00:00,  9.64it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[200]	valid_0&#39;s binary_logloss: 0.0042976	valid_1&#39;s binary_logloss: 0.126546
Early stopping, best iteration is:
[101]	valid_0&#39;s binary_logloss: 0.0220856	valid_1&#39;s binary_logloss: 0.0948469
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction_stage2, val_score: 0.084350:   0%|          | 0/6 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0175241	valid_1&#39;s binary_logloss: 0.0897803
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction_stage2, val_score: 0.084350:   0%|          | 0/6 [00:00&lt;?, ?it/s][I 2025-02-15 06:00:08,417] Trial 37 finished with value: 0.0875729614724119 and parameters: {&#39;feature_fraction&#39;: 0.8480000000000001}. Best is trial 37 with value: 0.0875729614724119.
feature_fraction_stage2, val_score: 0.084350:  17%|#6        | 1/6 [00:00&lt;00:00, 12.75it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[200]	valid_0&#39;s binary_logloss: 0.00275378	valid_1&#39;s binary_logloss: 0.100315
Early stopping, best iteration is:
[120]	valid_0&#39;s binary_logloss: 0.0119438	valid_1&#39;s binary_logloss: 0.087573
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0179948	valid_1&#39;s binary_logloss: 0.0924062
[200]	valid_0&#39;s binary_logloss: 0.00291026	valid_1&#39;s binary_logloss: 0.102757
Early stopping, best iteration is:
[144]	valid_0&#39;s binary_logloss: 0.00775066	valid_1&#39;s binary_logloss: 0.0903346
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction_stage2, val_score: 0.084350:  33%|###3      | 2/6 [00:00&lt;00:00, 10.85it/s][I 2025-02-15 06:00:08,531] Trial 38 finished with value: 0.09033459210811795 and parameters: {&#39;feature_fraction&#39;: 0.7520000000000001}. Best is trial 37 with value: 0.0875729614724119.
feature_fraction_stage2, val_score: 0.084350:  33%|###3      | 2/6 [00:00&lt;00:00, 10.85it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0175938	valid_1&#39;s binary_logloss: 0.0968142
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction_stage2, val_score: 0.084350:  33%|###3      | 2/6 [00:00&lt;00:00, 10.85it/s][I 2025-02-15 06:00:08,664] Trial 39 finished with value: 0.0941280619664432 and parameters: {&#39;feature_fraction&#39;: 0.88}. Best is trial 37 with value: 0.0875729614724119.
feature_fraction_stage2, val_score: 0.084350:  50%|#####     | 3/6 [00:00&lt;00:00, 10.85it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[200]	valid_0&#39;s binary_logloss: 0.00275023	valid_1&#39;s binary_logloss: 0.105479
Early stopping, best iteration is:
[145]	valid_0&#39;s binary_logloss: 0.00719252	valid_1&#39;s binary_logloss: 0.0941281
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0183159	valid_1&#39;s binary_logloss: 0.091937
[200]	valid_0&#39;s binary_logloss: 0.00296457	valid_1&#39;s binary_logloss: 0.0969964
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction_stage2, val_score: 0.084350:  67%|######6   | 4/6 [00:00&lt;00:00,  8.24it/s][I 2025-02-15 06:00:08,812] Trial 40 finished with value: 0.08744532173312741 and parameters: {&#39;feature_fraction&#39;: 0.7200000000000001}. Best is trial 40 with value: 0.08744532173312741.
feature_fraction_stage2, val_score: 0.084350:  67%|######6   | 4/6 [00:00&lt;00:00,  8.24it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[145]	valid_0&#39;s binary_logloss: 0.00763539	valid_1&#39;s binary_logloss: 0.0874453
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction_stage2, val_score: 0.084350:  83%|########3 | 5/6 [00:00&lt;00:00,  8.32it/s][I 2025-02-15 06:00:08,932] Trial 41 finished with value: 0.08435042633165667 and parameters: {&#39;feature_fraction&#39;: 0.784}. Best is trial 41 with value: 0.08435042633165667.
feature_fraction_stage2, val_score: 0.084350:  83%|########3 | 5/6 [00:00&lt;00:00,  8.32it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.0175414	valid_1&#39;s binary_logloss: 0.090751
[200]	valid_0&#39;s binary_logloss: 0.00273525	valid_1&#39;s binary_logloss: 0.0997948
Early stopping, best iteration is:
[145]	valid_0&#39;s binary_logloss: 0.00725553	valid_1&#39;s binary_logloss: 0.0843504
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0175414	valid_1&#39;s binary_logloss: 0.090751
[200]	valid_0&#39;s binary_logloss: 0.00273525	valid_1&#39;s binary_logloss: 0.0997948
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction_stage2, val_score: 0.084350: 100%|##########| 6/6 [00:00&lt;00:00,  8.72it/s][I 2025-02-15 06:00:09,036] Trial 42 finished with value: 0.08435042633165667 and parameters: {&#39;feature_fraction&#39;: 0.8160000000000001}. Best is trial 41 with value: 0.08435042633165667.
feature_fraction_stage2, val_score: 0.084350: 100%|##########| 6/6 [00:00&lt;00:00,  8.59it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[145]	valid_0&#39;s binary_logloss: 0.00725553	valid_1&#39;s binary_logloss: 0.0843504
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.084350:   0%|          | 0/20 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.084350:   0%|          | 0/20 [00:00&lt;?, ?it/s][I 2025-02-15 06:00:09,150] Trial 43 finished with value: 0.10378220155337738 and parameters: {&#39;lambda_l1&#39;: 2.7714891686764163, &#39;lambda_l2&#39;: 2.8708854998722266e-06}. Best is trial 43 with value: 0.10378220155337738.
regularization_factors, val_score: 0.084350:   5%|5         | 1/20 [00:00&lt;00:01,  9.62it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.0610745	valid_1&#39;s binary_logloss: 0.107728
[200]	valid_0&#39;s binary_logloss: 0.0534893	valid_1&#39;s binary_logloss: 0.103782
Early stopping, best iteration is:
[173]	valid_0&#39;s binary_logloss: 0.0534893	valid_1&#39;s binary_logloss: 0.103782
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0572828	valid_1&#39;s binary_logloss: 0.105651
[200]	valid_0&#39;s binary_logloss: 0.03158	valid_1&#39;s binary_logloss: 0.0914816
[300]	valid_0&#39;s binary_logloss: 0.0209858	valid_1&#39;s binary_logloss: 0.0889775
[400]	valid_0&#39;s binary_logloss: 0.0151547	valid_1&#39;s binary_logloss: 0.0895701
Early stopping, best iteration is:
[350]	valid_0&#39;s binary_logloss: 0.0177235	valid_1&#39;s binary_logloss: 0.0873283
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.084350:  10%|#         | 2/20 [00:00&lt;00:02,  7.46it/s][I 2025-02-15 06:00:09,320] Trial 44 finished with value: 0.08732827345509729 and parameters: {&#39;lambda_l1&#39;: 1.0890256845950948e-08, &#39;lambda_l2&#39;: 8.864741086087909}. Best is trial 44 with value: 0.08732827345509729.
regularization_factors, val_score: 0.084350:  10%|#         | 2/20 [00:00&lt;00:02,  7.46it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0534699	valid_1&#39;s binary_logloss: 0.102708
[200]	valid_0&#39;s binary_logloss: 0.028194	valid_1&#39;s binary_logloss: 0.0890597
[300]	valid_0&#39;s binary_logloss: 0.0181751	valid_1&#39;s binary_logloss: 0.088758
Early stopping, best iteration is:
[225]	valid_0&#39;s binary_logloss: 0.0250991	valid_1&#39;s binary_logloss: 0.0871597
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.084350:  15%|#5        | 3/20 [00:00&lt;00:03,  4.81it/s][I 2025-02-15 06:00:09,632] Trial 45 finished with value: 0.08715969599549338 and parameters: {&#39;lambda_l1&#39;: 1.5010974121248445e-08, &#39;lambda_l2&#39;: 7.310680598890752}. Best is trial 45 with value: 0.08715969599549338.
regularization_factors, val_score: 0.084350:  15%|#5        | 3/20 [00:00&lt;00:03,  4.81it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0466526	valid_1&#39;s binary_logloss: 0.101058
[200]	valid_0&#39;s binary_logloss: 0.0226416	valid_1&#39;s binary_logloss: 0.0901344
[300]	valid_0&#39;s binary_logloss: 0.0140011	valid_1&#39;s binary_logloss: 0.0904517
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.084350:  20%|##        | 4/20 [00:00&lt;00:04,  3.73it/s][I 2025-02-15 06:00:10,003] Trial 46 finished with value: 0.08900198077445268 and parameters: {&#39;lambda_l1&#39;: 2.430322492932399e-08, &#39;lambda_l2&#39;: 5.011411366367896}. Best is trial 45 with value: 0.08715969599549338.
regularization_factors, val_score: 0.084350:  20%|##        | 4/20 [00:00&lt;00:04,  3.73it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[262]	valid_0&#39;s binary_logloss: 0.0164916	valid_1&#39;s binary_logloss: 0.089002
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0176076	valid_1&#39;s binary_logloss: 0.0907344
[200]	valid_0&#39;s binary_logloss: 0.00280017	valid_1&#39;s binary_logloss: 0.098708
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.083766:  25%|##5       | 5/20 [00:02&lt;00:08,  1.75it/s][I 2025-02-15 06:00:11,136] Trial 47 finished with value: 0.0837657679292881 and parameters: {&#39;lambda_l1&#39;: 0.00012845420109235375, &#39;lambda_l2&#39;: 0.00446634677486112}. Best is trial 47 with value: 0.0837657679292881.
regularization_factors, val_score: 0.083766:  25%|##5       | 5/20 [00:02&lt;00:08,  1.75it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[145]	valid_0&#39;s binary_logloss: 0.00731558	valid_1&#39;s binary_logloss: 0.0837658
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.083766:  30%|###       | 6/20 [00:02&lt;00:08,  1.63it/s][I 2025-02-15 06:00:11,841] Trial 48 finished with value: 0.08433119533626658 and parameters: {&#39;lambda_l1&#39;: 0.0005181974873348894, &#39;lambda_l2&#39;: 0.0010841421657986174}. Best is trial 47 with value: 0.0837657679292881.
regularization_factors, val_score: 0.083766:  30%|###       | 6/20 [00:02&lt;00:08,  1.63it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.0175654	valid_1&#39;s binary_logloss: 0.0907455
[200]	valid_0&#39;s binary_logloss: 0.00276037	valid_1&#39;s binary_logloss: 0.0996527
Early stopping, best iteration is:
[145]	valid_0&#39;s binary_logloss: 0.00728169	valid_1&#39;s binary_logloss: 0.0843312
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.083766:  35%|###5      | 7/20 [00:03&lt;00:06,  2.00it/s][I 2025-02-15 06:00:12,100] Trial 49 finished with value: 0.08433344500630618 and parameters: {&#39;lambda_l1&#39;: 0.0008807328102774281, &#39;lambda_l2&#39;: 0.0006255507296244749}. Best is trial 47 with value: 0.0837657679292881.
regularization_factors, val_score: 0.083766:  35%|###5      | 7/20 [00:03&lt;00:06,  2.00it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0175647	valid_1&#39;s binary_logloss: 0.0907462
[200]	valid_0&#39;s binary_logloss: 0.00276069	valid_1&#39;s binary_logloss: 0.0996575
Early stopping, best iteration is:
[145]	valid_0&#39;s binary_logloss: 0.00728164	valid_1&#39;s binary_logloss: 0.0843334
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.083766:  40%|####      | 8/20 [00:03&lt;00:04,  2.66it/s][I 2025-02-15 06:00:12,203] Trial 50 finished with value: 0.08433024566860972 and parameters: {&#39;lambda_l1&#39;: 0.000793450229320218, &#39;lambda_l2&#39;: 0.0010320879954442014}. Best is trial 47 with value: 0.0837657679292881.
regularization_factors, val_score: 0.083766:  45%|####5     | 9/20 [00:03&lt;00:03,  3.44it/s][I 2025-02-15 06:00:12,305] Trial 51 finished with value: 0.08433320786270414 and parameters: {&#39;lambda_l1&#39;: 0.0005254154188814732, &#39;lambda_l2&#39;: 0.0008559024761066031}. Best is trial 47 with value: 0.0837657679292881.
regularization_factors, val_score: 0.083766:  45%|####5     | 9/20 [00:03&lt;00:03,  3.44it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.0175691	valid_1&#39;s binary_logloss: 0.0907449
[200]	valid_0&#39;s binary_logloss: 0.00277847	valid_1&#39;s binary_logloss: 0.100887
Early stopping, best iteration is:
[145]	valid_0&#39;s binary_logloss: 0.00728561	valid_1&#39;s binary_logloss: 0.0843302
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0175622	valid_1&#39;s binary_logloss: 0.0907463
[200]	valid_0&#39;s binary_logloss: 0.00275741	valid_1&#39;s binary_logloss: 0.0996712
Early stopping, best iteration is:
[145]	valid_0&#39;s binary_logloss: 0.00727875	valid_1&#39;s binary_logloss: 0.0843332
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0175637	valid_1&#39;s binary_logloss: 0.0907459
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.083766:  50%|#####     | 10/20 [00:03&lt;00:02,  4.29it/s][I 2025-02-15 06:00:12,409] Trial 52 finished with value: 0.08433186283705385 and parameters: {&#39;lambda_l1&#39;: 0.0004457263574754615, &#39;lambda_l2&#39;: 0.0010513719869144076}. Best is trial 47 with value: 0.0837657679292881.
regularization_factors, val_score: 0.083766:  50%|#####     | 10/20 [00:03&lt;00:02,  4.29it/s][I 2025-02-15 06:00:12,503] Trial 53 finished with value: 0.08433474520065698 and parameters: {&#39;lambda_l1&#39;: 0.0006585818204804891, &#39;lambda_l2&#39;: 0.0006084744006729451}. Best is trial 47 with value: 0.0837657679292881.
regularization_factors, val_score: 0.083766:  55%|#####5    | 11/20 [00:03&lt;00:02,  4.29it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[200]	valid_0&#39;s binary_logloss: 0.00275862	valid_1&#39;s binary_logloss: 0.0996622
Early stopping, best iteration is:
[145]	valid_0&#39;s binary_logloss: 0.00728003	valid_1&#39;s binary_logloss: 0.0843319
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0175608	valid_1&#39;s binary_logloss: 0.0907469
[200]	valid_0&#39;s binary_logloss: 0.00275646	valid_1&#39;s binary_logloss: 0.0996796
Early stopping, best iteration is:
[145]	valid_0&#39;s binary_logloss: 0.00727768	valid_1&#39;s binary_logloss: 0.0843347
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0175644	valid_1&#39;s binary_logloss: 0.0907458
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.083766:  60%|######    | 12/20 [00:03&lt;00:01,  5.54it/s][I 2025-02-15 06:00:12,653] Trial 54 finished with value: 0.08433174388265524 and parameters: {&#39;lambda_l1&#39;: 0.0005068878210749639, &#39;lambda_l2&#39;: 0.0010295153470835415}. Best is trial 47 with value: 0.0837657679292881.
regularization_factors, val_score: 0.083766:  65%|######5   | 13/20 [00:03&lt;00:01,  5.71it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[200]	valid_0&#39;s binary_logloss: 0.00275943	valid_1&#39;s binary_logloss: 0.0996584
Early stopping, best iteration is:
[145]	valid_0&#39;s binary_logloss: 0.00728077	valid_1&#39;s binary_logloss: 0.0843317
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0175819	valid_1&#39;s binary_logloss: 0.0907412
[200]	valid_0&#39;s binary_logloss: 0.00278947	valid_1&#39;s binary_logloss: 0.100807
Early stopping, best iteration is:
[145]	valid_0&#39;s binary_logloss: 0.00729684	valid_1&#39;s binary_logloss: 0.0843204
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[I 2025-02-15 06:00:12,811] Trial 55 finished with value: 0.08432036101674602 and parameters: {&#39;lambda_l1&#39;: 0.0004028971929676818, &#39;lambda_l2&#39;: 0.0023630119533374115}. Best is trial 47 with value: 0.0837657679292881.
regularization_factors, val_score: 0.083766:  70%|#######   | 14/20 [00:03&lt;00:00,  6.37it/s][I 2025-02-15 06:00:12,915] Trial 56 finished with value: 0.08823623962487215 and parameters: {&#39;lambda_l1&#39;: 0.00010757945858613374, &#39;lambda_l2&#39;: 0.008102013428414158}. Best is trial 47 with value: 0.0837657679292881.
regularization_factors, val_score: 0.083766:  75%|#######5  | 15/20 [00:03&lt;00:00,  7.01it/s][I 2025-02-15 06:00:13,022] Trial 57 finished with value: 0.09192028629218993 and parameters: {&#39;lambda_l1&#39;: 7.327033960602709e-05, &#39;lambda_l2&#39;: 0.00972047538669659}. Best is trial 47 with value: 0.0837657679292881.
regularization_factors, val_score: 0.083766:  75%|#######5  | 15/20 [00:03&lt;00:00,  7.01it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.017783	valid_1&#39;s binary_logloss: 0.0913093
[200]	valid_0&#39;s binary_logloss: 0.00286131	valid_1&#39;s binary_logloss: 0.0985716
Early stopping, best iteration is:
[142]	valid_0&#39;s binary_logloss: 0.00783381	valid_1&#39;s binary_logloss: 0.0882362
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0177484	valid_1&#39;s binary_logloss: 0.0972307
[200]	valid_0&#39;s binary_logloss: 0.00289453	valid_1&#39;s binary_logloss: 0.104277
Early stopping, best iteration is:
[145]	valid_0&#39;s binary_logloss: 0.00745171	valid_1&#39;s binary_logloss: 0.0919203
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.083766:  80%|########  | 16/20 [00:04&lt;00:00,  7.58it/s][I 2025-02-15 06:00:13,129] Trial 58 finished with value: 0.09171687698242909 and parameters: {&#39;lambda_l1&#39;: 0.005013064175582537, &#39;lambda_l2&#39;: 0.009117828873521102}. Best is trial 47 with value: 0.0837657679292881.
regularization_factors, val_score: 0.083766:  85%|########5 | 17/20 [00:04&lt;00:00,  8.00it/s][I 2025-02-15 06:00:13,234] Trial 59 finished with value: 0.08434937705155016 and parameters: {&#39;lambda_l1&#39;: 2.9800072507817458e-05, &#39;lambda_l2&#39;: 0.00010062436387969577}. Best is trial 47 with value: 0.0837657679292881.
regularization_factors, val_score: 0.083766:  85%|########5 | 17/20 [00:04&lt;00:00,  8.00it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0178289	valid_1&#39;s binary_logloss: 0.0974641
[200]	valid_0&#39;s binary_logloss: 0.00294678	valid_1&#39;s binary_logloss: 0.0993165
Early stopping, best iteration is:
[145]	valid_0&#39;s binary_logloss: 0.00762767	valid_1&#39;s binary_logloss: 0.0917169
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0175434	valid_1&#39;s binary_logloss: 0.0907505
[200]	valid_0&#39;s binary_logloss: 0.00273715	valid_1&#39;s binary_logloss: 0.0997834
Early stopping, best iteration is:
[145]	valid_0&#39;s binary_logloss: 0.00725738	valid_1&#39;s binary_logloss: 0.0843494
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.083766:  90%|######### | 18/20 [00:04&lt;00:00,  8.40it/s][I 2025-02-15 06:00:13,336] Trial 60 finished with value: 0.08870552038122105 and parameters: {&#39;lambda_l1&#39;: 0.012574372964787266, &#39;lambda_l2&#39;: 0.005230958760581126}. Best is trial 47 with value: 0.0837657679292881.
regularization_factors, val_score: 0.083766:  95%|#########5| 19/20 [00:04&lt;00:00,  8.73it/s][I 2025-02-15 06:00:13,440] Trial 61 finished with value: 0.08433282721563788 and parameters: {&#39;lambda_l1&#39;: 0.0005335764298017957, &#39;lambda_l2&#39;: 0.000893557463942489}. Best is trial 47 with value: 0.0837657679292881.
regularization_factors, val_score: 0.083766:  95%|#########5| 19/20 [00:04&lt;00:00,  8.73it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0179078	valid_1&#39;s binary_logloss: 0.0916347
[200]	valid_0&#39;s binary_logloss: 0.00305503	valid_1&#39;s binary_logloss: 0.102509
Early stopping, best iteration is:
[143]	valid_0&#39;s binary_logloss: 0.00801656	valid_1&#39;s binary_logloss: 0.0887055
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0175629	valid_1&#39;s binary_logloss: 0.0907462
[200]	valid_0&#39;s binary_logloss: 0.00275807	valid_1&#39;s binary_logloss: 0.0996673
Early stopping, best iteration is:
[145]	valid_0&#39;s binary_logloss: 0.00727939	valid_1&#39;s binary_logloss: 0.0843328
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.083766: 100%|##########| 20/20 [00:04&lt;00:00,  8.77it/s][I 2025-02-15 06:00:13,555] Trial 62 finished with value: 0.08433300789953678 and parameters: {&#39;lambda_l1&#39;: 0.0018141096262519335, &#39;lambda_l2&#39;: 0.00013850761755928952}. Best is trial 47 with value: 0.0837657679292881.
regularization_factors, val_score: 0.083766: 100%|##########| 20/20 [00:04&lt;00:00,  4.43it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0175727	valid_1&#39;s binary_logloss: 0.0907453
[200]	valid_0&#39;s binary_logloss: 0.00278468	valid_1&#39;s binary_logloss: 0.100863
Early stopping, best iteration is:
[145]	valid_0&#39;s binary_logloss: 0.0072908	valid_1&#39;s binary_logloss: 0.084333
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>min_child_samples, val_score: 0.083766:   0%|          | 0/5 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0204314	valid_1&#39;s binary_logloss: 0.0918761
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>min_child_samples, val_score: 0.083766:  20%|##        | 1/5 [00:00&lt;00:00,  6.33it/s][I 2025-02-15 06:00:13,729] Trial 63 finished with value: 0.08936496652508225 and parameters: {&#39;min_child_samples&#39;: 50}. Best is trial 63 with value: 0.08936496652508225.
min_child_samples, val_score: 0.083766:  20%|##        | 1/5 [00:00&lt;00:00,  6.33it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[200]	valid_0&#39;s binary_logloss: 0.00359615	valid_1&#39;s binary_logloss: 0.0992154
Early stopping, best iteration is:
[145]	valid_0&#39;s binary_logloss: 0.00902222	valid_1&#39;s binary_logloss: 0.089365
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>min_child_samples, val_score: 0.082978:  40%|####      | 2/5 [00:00&lt;00:00,  6.47it/s][I 2025-02-15 06:00:13,882] Trial 64 finished with value: 0.0829779141973636 and parameters: {&#39;min_child_samples&#39;: 100}. Best is trial 64 with value: 0.0829779141973636.
min_child_samples, val_score: 0.082978:  40%|####      | 2/5 [00:00&lt;00:00,  6.47it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.0317388	valid_1&#39;s binary_logloss: 0.0854332
[200]	valid_0&#39;s binary_logloss: 0.00827085	valid_1&#39;s binary_logloss: 0.090576
Early stopping, best iteration is:
[105]	valid_0&#39;s binary_logloss: 0.0292856	valid_1&#39;s binary_logloss: 0.0829779
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>min_child_samples, val_score: 0.082978:  60%|######    | 3/5 [00:00&lt;00:00,  7.52it/s][I 2025-02-15 06:00:13,989] Trial 65 finished with value: 0.090959007468722 and parameters: {&#39;min_child_samples&#39;: 5}. Best is trial 64 with value: 0.0829779141973636.
min_child_samples, val_score: 0.082978:  60%|######    | 3/5 [00:00&lt;00:00,  7.52it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.016496	valid_1&#39;s binary_logloss: 0.0940638
[200]	valid_0&#39;s binary_logloss: 0.00272822	valid_1&#39;s binary_logloss: 0.106572
Early stopping, best iteration is:
[148]	valid_0&#39;s binary_logloss: 0.00679923	valid_1&#39;s binary_logloss: 0.090959
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>min_child_samples, val_score: 0.082978:  80%|########  | 4/5 [00:00&lt;00:00,  8.28it/s][I 2025-02-15 06:00:14,090] Trial 66 finished with value: 0.09514467481654297 and parameters: {&#39;min_child_samples&#39;: 10}. Best is trial 64 with value: 0.0829779141973636.
min_child_samples, val_score: 0.082978:  80%|########  | 4/5 [00:00&lt;00:00,  8.28it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.0170212	valid_1&#39;s binary_logloss: 0.098272
[200]	valid_0&#39;s binary_logloss: 0.00265659	valid_1&#39;s binary_logloss: 0.110991
Early stopping, best iteration is:
[144]	valid_0&#39;s binary_logloss: 0.00702166	valid_1&#39;s binary_logloss: 0.0951447
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>min_child_samples, val_score: 0.082978:  80%|########  | 4/5 [00:00&lt;00:00,  8.28it/s][I 2025-02-15 06:00:14,183] Trial 67 finished with value: 0.09491639388859692 and parameters: {&#39;min_child_samples&#39;: 25}. Best is trial 64 with value: 0.0829779141973636.
min_child_samples, val_score: 0.082978: 100%|##########| 5/5 [00:00&lt;00:00,  8.07it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0178567	valid_1&#39;s binary_logloss: 0.100867
[200]	valid_0&#39;s binary_logloss: 0.00293044	valid_1&#39;s binary_logloss: 0.104825
Early stopping, best iteration is:
[144]	valid_0&#39;s binary_logloss: 0.00777149	valid_1&#39;s binary_logloss: 0.0949164
</pre></div>
</div>
</div>
</div>
</section>
<section id="optuna-lightgbm">
<h2>optuna.lightgbm 再現性<a class="headerlink" href="#optuna-lightgbm" title="Link to this heading">#</a></h2>
<p>optuna-integrationを使う場合、lightGBMの各種オプションに加え、<br />
optuna_seedオプションを指定することで、再現性が確保できそうです。</p>
<p><a class="reference external" href="https://optuna.readthedocs.io/en/v3.5.0/reference/generated/optuna.integration.lightgbm.train.html">https://optuna.readthedocs.io/en/v3.5.0/reference/generated/optuna.integration.lightgbm.train.html</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">optuna.integration</span><span class="w"> </span><span class="kn">import</span> <span class="n">lightgbm</span> <span class="k">as</span> <span class="n">lgb</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.datasets</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">optuna</span>
<span class="n">optuna</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_x</span><span class="p">,</span> <span class="n">val_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">val_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">dtrain</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">train_y</span><span class="p">)</span>
<span class="n">dval</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">val_x</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">val_y</span><span class="p">)</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;objective&quot;</span><span class="p">:</span> <span class="s2">&quot;binary&quot;</span><span class="p">,</span>
    <span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="s2">&quot;binary_logloss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;verbosity&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>

    <span class="c1"># LightGBMの再現性確保</span>
    <span class="s2">&quot;seed&quot;</span><span class="p">:</span> <span class="mi">42</span><span class="p">,</span>
    <span class="s2">&quot;deterministic&quot;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span>
    <span class="s2">&quot;force_col_wise&quot;</span><span class="p">:</span><span class="kc">True</span>
<span class="p">}</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
    <span class="n">params</span><span class="p">,</span>
    <span class="n">dtrain</span><span class="p">,</span>
    <span class="n">valid_sets</span><span class="o">=</span><span class="p">[</span><span class="n">dtrain</span><span class="p">,</span> <span class="n">dval</span><span class="p">],</span>
    <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">lgb</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">lgb</span><span class="o">.</span><span class="n">log_evaluation</span><span class="p">(</span><span class="mi">100</span><span class="p">)],</span>
    <span class="n">optuna_seed</span><span class="o">=</span><span class="mi">42</span> <span class="c1"># optunaの再現性確保</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction, val_score: inf:   0%|          | 0/7 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000798837	valid_1&#39;s binary_logloss: 0.0931527
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction, val_score: 0.079337:  14%|#4        | 1/7 [00:00&lt;00:00,  8.51it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[50]	valid_0&#39;s binary_logloss: 0.0173087	valid_1&#39;s binary_logloss: 0.0793373
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000662867	valid_1&#39;s binary_logloss: 0.123845
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction, val_score: 0.079337:  29%|##8       | 2/7 [00:00&lt;00:00,  8.28it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[50]	valid_0&#39;s binary_logloss: 0.0159317	valid_1&#39;s binary_logloss: 0.102513
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000540664	valid_1&#39;s binary_logloss: 0.109988
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction, val_score: 0.079337:  43%|####2     | 3/7 [00:00&lt;00:00,  5.78it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[55]	valid_0&#39;s binary_logloss: 0.0105048	valid_1&#39;s binary_logloss: 0.103744
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000627546	valid_1&#39;s binary_logloss: 0.109223
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction, val_score: 0.079337:  57%|#####7    | 4/7 [00:00&lt;00:00,  5.11it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[55]	valid_0&#39;s binary_logloss: 0.0111996	valid_1&#39;s binary_logloss: 0.0933413
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000506273	valid_1&#39;s binary_logloss: 0.115074
Early stopping, best iteration is:
[49]	valid_0&#39;s binary_logloss: 0.0155274	valid_1&#39;s binary_logloss: 0.0966291
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction, val_score: 0.079337:  71%|#######1  | 5/7 [00:00&lt;00:00,  5.53it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction, val_score: 0.079337:  86%|########5 | 6/7 [00:01&lt;00:00,  5.76it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000548261	valid_1&#39;s binary_logloss: 0.0971404
Early stopping, best iteration is:
[51]	valid_0&#39;s binary_logloss: 0.0141151	valid_1&#39;s binary_logloss: 0.0913298
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000462198	valid_1&#39;s binary_logloss: 0.12469
Early stopping, best iteration is:
[41]	valid_0&#39;s binary_logloss: 0.02643	valid_1&#39;s binary_logloss: 0.103654
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction, val_score: 0.079337: 100%|##########| 7/7 [00:01&lt;00:00,  5.88it/s]
num_leaves, val_score: 0.079337:   5%|5         | 1/20 [00:00&lt;00:02,  8.44it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000798837	valid_1&#39;s binary_logloss: 0.0931527
Early stopping, best iteration is:
[50]	valid_0&#39;s binary_logloss: 0.0173087	valid_1&#39;s binary_logloss: 0.0793373
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.079337:  15%|#5        | 3/20 [00:00&lt;00:02,  6.71it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000798837	valid_1&#39;s binary_logloss: 0.0931527
Early stopping, best iteration is:
[50]	valid_0&#39;s binary_logloss: 0.0173087	valid_1&#39;s binary_logloss: 0.0793373
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000798837	valid_1&#39;s binary_logloss: 0.0931527
Early stopping, best iteration is:
[50]	valid_0&#39;s binary_logloss: 0.0173087	valid_1&#39;s binary_logloss: 0.0793373
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.079337:  25%|##5       | 5/20 [00:00&lt;00:01,  8.28it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000761943	valid_1&#39;s binary_logloss: 0.0942305
Early stopping, best iteration is:
[47]	valid_0&#39;s binary_logloss: 0.0210427	valid_1&#39;s binary_logloss: 0.0824628
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000798837	valid_1&#39;s binary_logloss: 0.0931527
Early stopping, best iteration is:
[50]	valid_0&#39;s binary_logloss: 0.0173087	valid_1&#39;s binary_logloss: 0.0793373
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.079337:  35%|###5      | 7/20 [00:00&lt;00:01,  8.18it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000798837	valid_1&#39;s binary_logloss: 0.0931527
Early stopping, best iteration is:
[50]	valid_0&#39;s binary_logloss: 0.0173087	valid_1&#39;s binary_logloss: 0.0793373
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000798837	valid_1&#39;s binary_logloss: 0.0931527
Early stopping, best iteration is:
[50]	valid_0&#39;s binary_logloss: 0.0173087	valid_1&#39;s binary_logloss: 0.0793373
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.079337:  45%|####5     | 9/20 [00:01&lt;00:01,  7.93it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000798837	valid_1&#39;s binary_logloss: 0.0931527
Early stopping, best iteration is:
[50]	valid_0&#39;s binary_logloss: 0.0173087	valid_1&#39;s binary_logloss: 0.0793373
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000798837	valid_1&#39;s binary_logloss: 0.0931527
Early stopping, best iteration is:
[50]	valid_0&#39;s binary_logloss: 0.0173087	valid_1&#39;s binary_logloss: 0.0793373
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.079337:  55%|#####5    | 11/20 [00:01&lt;00:01,  6.81it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000798837	valid_1&#39;s binary_logloss: 0.0931527
Early stopping, best iteration is:
[50]	valid_0&#39;s binary_logloss: 0.0173087	valid_1&#39;s binary_logloss: 0.0793373
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000798837	valid_1&#39;s binary_logloss: 0.0931527
Early stopping, best iteration is:
[50]	valid_0&#39;s binary_logloss: 0.0173087	valid_1&#39;s binary_logloss: 0.0793373
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.079337:  60%|######    | 12/20 [00:01&lt;00:01,  7.19it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0008011	valid_1&#39;s binary_logloss: 0.0918996
Early stopping, best iteration is:
[50]	valid_0&#39;s binary_logloss: 0.0173089	valid_1&#39;s binary_logloss: 0.0793396
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000798837	valid_1&#39;s binary_logloss: 0.0931527
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.079337:  70%|#######   | 14/20 [00:01&lt;00:00,  7.64it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[50]	valid_0&#39;s binary_logloss: 0.0173087	valid_1&#39;s binary_logloss: 0.0793373
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000798837	valid_1&#39;s binary_logloss: 0.0931527
Early stopping, best iteration is:
[50]	valid_0&#39;s binary_logloss: 0.0173087	valid_1&#39;s binary_logloss: 0.0793373
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.079337:  80%|########  | 16/20 [00:02&lt;00:00,  7.65it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000798837	valid_1&#39;s binary_logloss: 0.0931527
Early stopping, best iteration is:
[50]	valid_0&#39;s binary_logloss: 0.0173087	valid_1&#39;s binary_logloss: 0.0793373
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000798837	valid_1&#39;s binary_logloss: 0.0931527
Early stopping, best iteration is:
[50]	valid_0&#39;s binary_logloss: 0.0173087	valid_1&#39;s binary_logloss: 0.0793373
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.079337:  90%|######### | 18/20 [00:02&lt;00:00,  6.84it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000798837	valid_1&#39;s binary_logloss: 0.0931527
Early stopping, best iteration is:
[50]	valid_0&#39;s binary_logloss: 0.0173087	valid_1&#39;s binary_logloss: 0.0793373
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000798837	valid_1&#39;s binary_logloss: 0.0931527
Early stopping, best iteration is:
[50]	valid_0&#39;s binary_logloss: 0.0173087	valid_1&#39;s binary_logloss: 0.0793373
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.079337:  95%|#########5| 19/20 [00:02&lt;00:00,  6.75it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.000798837	valid_1&#39;s binary_logloss: 0.0931527
Early stopping, best iteration is:
[50]	valid_0&#39;s binary_logloss: 0.0173087	valid_1&#39;s binary_logloss: 0.0793373
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>num_leaves, val_score: 0.079337: 100%|##########| 20/20 [00:02&lt;00:00,  7.31it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.000798837	valid_1&#39;s binary_logloss: 0.0931527
Early stopping, best iteration is:
[50]	valid_0&#39;s binary_logloss: 0.0173087	valid_1&#39;s binary_logloss: 0.0793373
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bagging, val_score: 0.076877:  10%|#         | 1/10 [00:00&lt;00:00,  9.14it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00241844	valid_1&#39;s binary_logloss: 0.0949912
Early stopping, best iteration is:
[59]	valid_0&#39;s binary_logloss: 0.0167482	valid_1&#39;s binary_logloss: 0.0768767
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bagging, val_score: 0.069340:  20%|##        | 2/10 [00:00&lt;00:00,  9.31it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.0027699	valid_1&#39;s binary_logloss: 0.0899105
Early stopping, best iteration is:
[59]	valid_0&#39;s binary_logloss: 0.0177185	valid_1&#39;s binary_logloss: 0.0693396
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bagging, val_score: 0.069340:  30%|###       | 3/10 [00:00&lt;00:00,  8.50it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0026666	valid_1&#39;s binary_logloss: 0.0920643
Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.008314	valid_1&#39;s binary_logloss: 0.0773779
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00305274	valid_1&#39;s binary_logloss: 0.0762157
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bagging, val_score: 0.066848:  40%|####      | 4/10 [00:00&lt;00:00,  7.40it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[76]	valid_0&#39;s binary_logloss: 0.008951	valid_1&#39;s binary_logloss: 0.0668483
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00287452	valid_1&#39;s binary_logloss: 0.0805357
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bagging, val_score: 0.063744:  50%|#####     | 5/10 [00:00&lt;00:00,  7.08it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[77]	valid_0&#39;s binary_logloss: 0.00826191	valid_1&#39;s binary_logloss: 0.0637444
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bagging, val_score: 0.063744:  60%|######    | 6/10 [00:00&lt;00:00,  7.41it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.00304696	valid_1&#39;s binary_logloss: 0.0941191
Early stopping, best iteration is:
[65]	valid_0&#39;s binary_logloss: 0.0146562	valid_1&#39;s binary_logloss: 0.0752165
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bagging, val_score: 0.063744:  70%|#######   | 7/10 [00:00&lt;00:00,  7.59it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.00306808	valid_1&#39;s binary_logloss: 0.0744141
Early stopping, best iteration is:
[70]	valid_0&#39;s binary_logloss: 0.0113338	valid_1&#39;s binary_logloss: 0.0639117
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bagging, val_score: 0.063744:  80%|########  | 8/10 [00:01&lt;00:00,  7.88it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.00285912	valid_1&#39;s binary_logloss: 0.0800863
Early stopping, best iteration is:
[70]	valid_0&#39;s binary_logloss: 0.0110939	valid_1&#39;s binary_logloss: 0.0687253
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bagging, val_score: 0.057978:  90%|######### | 9/10 [00:01&lt;00:00,  8.24it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.006777	valid_1&#39;s binary_logloss: 0.0683311
Early stopping, best iteration is:
[80]	valid_0&#39;s binary_logloss: 0.0140155	valid_1&#39;s binary_logloss: 0.0579785
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>bagging, val_score: 0.057978: 100%|##########| 10/10 [00:01&lt;00:00,  8.13it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.00919746	valid_1&#39;s binary_logloss: 0.0824783
Early stopping, best iteration is:
[87]	valid_0&#39;s binary_logloss: 0.0131203	valid_1&#39;s binary_logloss: 0.0724166
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction_stage2, val_score: 0.057978:   0%|          | 0/3 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.006777	valid_1&#39;s binary_logloss: 0.0683311
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction_stage2, val_score: 0.057978:  33%|###3      | 1/3 [00:00&lt;00:00,  9.67it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[80]	valid_0&#39;s binary_logloss: 0.0140155	valid_1&#39;s binary_logloss: 0.0579785
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00662665	valid_1&#39;s binary_logloss: 0.0613112
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction_stage2, val_score: 0.056607:  67%|######6   | 2/3 [00:00&lt;00:00,  8.03it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[82]	valid_0&#39;s binary_logloss: 0.0134534	valid_1&#39;s binary_logloss: 0.0566075
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00662703	valid_1&#39;s binary_logloss: 0.0767907
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>feature_fraction_stage2, val_score: 0.056607: 100%|##########| 3/3 [00:00&lt;00:00,  7.34it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[62]	valid_0&#39;s binary_logloss: 0.0243423	valid_1&#39;s binary_logloss: 0.064483
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.056607:   5%|5         | 1/20 [00:00&lt;00:02,  9.49it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00642998	valid_1&#39;s binary_logloss: 0.0706322
Early stopping, best iteration is:
[86]	valid_0&#39;s binary_logloss: 0.0119747	valid_1&#39;s binary_logloss: 0.061859
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00663056	valid_1&#39;s binary_logloss: 0.0607414
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.056607:  15%|#5        | 3/20 [00:00&lt;00:02,  8.28it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[73]	valid_0&#39;s binary_logloss: 0.0181905	valid_1&#39;s binary_logloss: 0.0574405
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00609425	valid_1&#39;s binary_logloss: 0.0750981
Early stopping, best iteration is:
[86]	valid_0&#39;s binary_logloss: 0.0107853	valid_1&#39;s binary_logloss: 0.0649818
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00658002	valid_1&#39;s binary_logloss: 0.0596825
Early stopping, best iteration is:
[82]	valid_0&#39;s binary_logloss: 0.013456	valid_1&#39;s binary_logloss: 0.0557765
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.055777:  25%|##5       | 5/20 [00:00&lt;00:01,  7.95it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0584335	valid_1&#39;s binary_logloss: 0.0795187
[200]	valid_0&#39;s binary_logloss: 0.0322554	valid_1&#39;s binary_logloss: 0.0726083
[300]	valid_0&#39;s binary_logloss: 0.0211851	valid_1&#39;s binary_logloss: 0.0714622
[400]	valid_0&#39;s binary_logloss: 0.0146966	valid_1&#39;s binary_logloss: 0.0714926
Early stopping, best iteration is:
[390]	valid_0&#39;s binary_logloss: 0.0151744	valid_1&#39;s binary_logloss: 0.0707817
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00866234	valid_1&#39;s binary_logloss: 0.0629185
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.055614:  35%|###5      | 7/20 [00:00&lt;00:01,  8.34it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[79]	valid_0&#39;s binary_logloss: 0.0154558	valid_1&#39;s binary_logloss: 0.0599451
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00668709	valid_1&#39;s binary_logloss: 0.0619888
Early stopping, best iteration is:
[82]	valid_0&#39;s binary_logloss: 0.0134611	valid_1&#39;s binary_logloss: 0.0556138
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.055614:  40%|####      | 8/20 [00:01&lt;00:01,  8.05it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00668694	valid_1&#39;s binary_logloss: 0.0619914
Early stopping, best iteration is:
[82]	valid_0&#39;s binary_logloss: 0.0134612	valid_1&#39;s binary_logloss: 0.0556172
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00683944	valid_1&#39;s binary_logloss: 0.0771474
Early stopping, best iteration is:
[59]	valid_0&#39;s binary_logloss: 0.0269982	valid_1&#39;s binary_logloss: 0.0685674
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.055614:  55%|#####5    | 11/20 [00:01&lt;00:01,  8.60it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00668731	valid_1&#39;s binary_logloss: 0.0619915
Early stopping, best iteration is:
[82]	valid_0&#39;s binary_logloss: 0.013462	valid_1&#39;s binary_logloss: 0.0556172
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00636532	valid_1&#39;s binary_logloss: 0.0816198
Early stopping, best iteration is:
[59]	valid_0&#39;s binary_logloss: 0.0274406	valid_1&#39;s binary_logloss: 0.0661223
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.055614:  65%|######5   | 13/20 [00:01&lt;00:00,  8.86it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00664893	valid_1&#39;s binary_logloss: 0.0671045
Early stopping, best iteration is:
[71]	valid_0&#39;s binary_logloss: 0.0189805	valid_1&#39;s binary_logloss: 0.0605715
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00640395	valid_1&#39;s binary_logloss: 0.0749016
Early stopping, best iteration is:
[65]	valid_0&#39;s binary_logloss: 0.0216738	valid_1&#39;s binary_logloss: 0.0655627
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.055614:  75%|#######5  | 15/20 [00:01&lt;00:00,  9.17it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00636514	valid_1&#39;s binary_logloss: 0.0816202
Early stopping, best iteration is:
[59]	valid_0&#39;s binary_logloss: 0.0274403	valid_1&#39;s binary_logloss: 0.0661222
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00615533	valid_1&#39;s binary_logloss: 0.0685983
Early stopping, best iteration is:
[64]	valid_0&#39;s binary_logloss: 0.0236416	valid_1&#39;s binary_logloss: 0.0607408
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.055614:  85%|########5 | 17/20 [00:01&lt;00:00,  8.55it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.00655956	valid_1&#39;s binary_logloss: 0.0723242
Early stopping, best iteration is:
[71]	valid_0&#39;s binary_logloss: 0.0193484	valid_1&#39;s binary_logloss: 0.0587376
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00640185	valid_1&#39;s binary_logloss: 0.0739005
Early stopping, best iteration is:
[80]	valid_0&#39;s binary_logloss: 0.0138723	valid_1&#39;s binary_logloss: 0.0640537
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.055614:  95%|#########5| 19/20 [00:02&lt;00:00,  9.32it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00637029	valid_1&#39;s binary_logloss: 0.0717661
Early stopping, best iteration is:
[71]	valid_0&#39;s binary_logloss: 0.0186116	valid_1&#39;s binary_logloss: 0.0618947
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00672233	valid_1&#39;s binary_logloss: 0.0782671
Early stopping, best iteration is:
[59]	valid_0&#39;s binary_logloss: 0.0269994	valid_1&#39;s binary_logloss: 0.0685533
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>regularization_factors, val_score: 0.055614: 100%|##########| 20/20 [00:02&lt;00:00,  8.60it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.00621398	valid_1&#39;s binary_logloss: 0.0652893
Early stopping, best iteration is:
[71]	valid_0&#39;s binary_logloss: 0.01958	valid_1&#39;s binary_logloss: 0.0591574
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>min_child_samples, val_score: 0.055614:   0%|          | 0/5 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.0330963	valid_1&#39;s binary_logloss: 0.0563453
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>min_child_samples, val_score: 0.048403:  20%|##        | 1/5 [00:00&lt;00:00,  5.74it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[200]	valid_0&#39;s binary_logloss: 0.00736716	valid_1&#39;s binary_logloss: 0.0535529
Early stopping, best iteration is:
[142]	valid_0&#39;s binary_logloss: 0.0184855	valid_1&#39;s binary_logloss: 0.0484033
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>min_child_samples, val_score: 0.048403:  40%|####      | 2/5 [00:01&lt;00:03,  1.05s/it]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.00325198	valid_1&#39;s binary_logloss: 0.066742
Early stopping, best iteration is:
[62]	valid_0&#39;s binary_logloss: 0.0145426	valid_1&#39;s binary_logloss: 0.0537361
Training until validation scores don&#39;t improve for 100 rounds
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>min_child_samples, val_score: 0.048403:  80%|########  | 4/5 [00:02&lt;00:00,  1.27it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[100]	valid_0&#39;s binary_logloss: 0.00263509	valid_1&#39;s binary_logloss: 0.0772649
Early stopping, best iteration is:
[59]	valid_0&#39;s binary_logloss: 0.0122224	valid_1&#39;s binary_logloss: 0.0556671
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.00833215	valid_1&#39;s binary_logloss: 0.0692418
Early stopping, best iteration is:
[80]	valid_0&#39;s binary_logloss: 0.0180871	valid_1&#39;s binary_logloss: 0.0598457
Training until validation scores don&#39;t improve for 100 rounds
[100]	valid_0&#39;s binary_logloss: 0.137248	valid_1&#39;s binary_logloss: 0.0976008
[200]	valid_0&#39;s binary_logloss: 0.102815	valid_1&#39;s binary_logloss: 0.0749786
[300]	valid_0&#39;s binary_logloss: 0.0894889	valid_1&#39;s binary_logloss: 0.0738755
[400]	valid_0&#39;s binary_logloss: 0.0790843	valid_1&#39;s binary_logloss: 0.0757856
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>min_child_samples, val_score: 0.048403: 100%|##########| 5/5 [00:02&lt;00:00,  2.00it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Early stopping, best iteration is:
[345]	valid_0&#39;s binary_logloss: 0.0832511	valid_1&#39;s binary_logloss: 0.0658192
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">params</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;objective&#39;: &#39;binary&#39;,
 &#39;metric&#39;: &#39;binary_logloss&#39;,
 &#39;verbosity&#39;: -1,
 &#39;seed&#39;: 42,
 &#39;deterministic&#39;: True,
 &#39;force_col_wise&#39;: True,
 &#39;feature_pre_filter&#39;: False,
 &#39;lambda_l1&#39;: 1.9325731151388464e-06,
 &#39;lambda_l2&#39;: 4.26723135526715e-06,
 &#39;num_leaves&#39;: 31,
 &#39;feature_fraction&#39;: 0.44800000000000006,
 &#39;bagging_fraction&#39;: 0.47188117863746926,
 &#39;bagging_freq&#39;: 1,
 &#39;min_child_samples&#39;: 50,
 &#39;num_iterations&#39;: 1000}
</pre></div>
</div>
</div>
</div>
</section>
<section id="optuna-inetgration-scikit-learn-api">
<h2>optuna.inetgration scikit-learn API<a class="headerlink" href="#optuna-inetgration-scikit-learn-api" title="Link to this heading">#</a></h2>
<p>optuna-integrationで、LGBMClassifierがimportできたので、<br />
sklearn APIでも自動チューニングできるかと思いきや、どうやらsklearn APIは対応してないみたいでした。</p>
<p>ソースコード<br />
<a class="github reference external" href="https://github.com/optuna/optuna-integration/blob/main/optuna_integration/_lightgbm_tuner/sklearn.py">optuna/optuna-integration</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">optuna.integration.lightgbm</span><span class="w"> </span><span class="kn">import</span> <span class="n">LGBMClassifier</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LGBMClassifier</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.11/dist-packages/optuna_integration/_lightgbm_tuner/sklearn.py:30: UserWarning: LightGBMTuner doesn&#39;t support sklearn API. Use `train()` or `LightGBMTuner` for hyperparameter tuning.
  warnings.warn(
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="plotly_tips_collection.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Plotly Tips Collection</p>
      </div>
    </a>
    <a class="right-next"
       href="shap_handbook.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">shap</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">LightGBM Handbook</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">基本</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#python-api">Python API</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">パラメータ</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lightgbm-train">lightgbm.train：学習してモデル作成</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#booster">Booster：モデル</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lightgbm-cv">lightgbm.cv：交差検証</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cvbooster">CVBooster：交差検証時のモデル群</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scikit-learn-api">scikit-learn API</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">注意点</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">2種類の重要度</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">欠損の扱い</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">再現性の確保</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">バージョンによる違い</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">可視化などの便利関数</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-metric">plot_metric：評価値の推移の可視化</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-api">train APIの場合</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sklearn-api">sklearn APIの場合</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-importance">plot_importance：重要度の可視化</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-split-value-histogram">plot_split_value_histogram：分岐に使われる値の可視化</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#get-split-value-histogram">get_split_value_histogram：分岐に使われる値の取得</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-tree">plot_tree：決定木の可視化</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trees-to-dataframe">trees_to_dataframe：決定木をテーブル形式で取得</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#optuna">Optuna：ハイパーパラメータ自動チューニング</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lightgbmtuner">LightGBMTuner</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lightgbmtunercv">LightGBMTunerCV</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optuna-integration-lightgbm-tain">optuna.integration.lightgbm.tain</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optuna-lightgbm">optuna.lightgbm 再現性</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optuna-inetgration-scikit-learn-api">optuna.inetgration scikit-learn API</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By abay
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>